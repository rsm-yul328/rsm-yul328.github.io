[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cecelia Liu",
    "section": "",
    "text": "Here’s a little bit about me!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "HW 1/hw1_questions.html",
    "href": "HW 1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo briefly summarize the design: 50,083 past donors were randomly assigned to one of two groups—a control group (which received a standard appeal) and a treatment group (which received a matching offer). Within the treatment group, donors were further randomly assigned to one of several specific conditions: - Match ratios: 1:1, 2:1, or 3:1 (e.g., a $50 donation with a 2:1 match results in $150 total) - Match thresholds: limits of $25,000, $50,000, $100,000, or no cap - Suggested donation amounts: equal to, 1.25×, or 1.5× the donor’s previous maximum gift\nThe core message and layout of each letter remained the same—only the matching and suggestion elements varied. The goal was to test how these subtle framing differences affected both whether people chose to donate and how much they gave.\nIn this project, I replicate their study using the original dataset, examining both the balance across groups and the causal impact of different treatments on giving behavior."
  },
  {
    "objectID": "HW 1/hw1_questions.html#introduction",
    "href": "HW 1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo briefly summarize the design: 50,083 past donors were randomly assigned to one of two groups—a control group (which received a standard appeal) and a treatment group (which received a matching offer). Within the treatment group, donors were further randomly assigned to one of several specific conditions: - Match ratios: 1:1, 2:1, or 3:1 (e.g., a $50 donation with a 2:1 match results in $150 total) - Match thresholds: limits of $25,000, $50,000, $100,000, or no cap - Suggested donation amounts: equal to, 1.25×, or 1.5× the donor’s previous maximum gift\nThe core message and layout of each letter remained the same—only the matching and suggestion elements varied. The goal was to test how these subtle framing differences affected both whether people chose to donate and how much they gave.\nIn this project, I replicate their study using the original dataset, examining both the balance across groups and the causal impact of different treatments on giving behavior."
  },
  {
    "objectID": "HW 1/hw1_questions.html#data",
    "href": "HW 1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)  \nlibrary(dplyr)    \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  \n\nThe dataset includes 50,083 observations and 51 variables. Each row corresponds to a past donor who was randomly assigned to receive one of several fundraising letters as part of Karlan and List’s experiment.\nBelow is a preview of the first 10 rows:\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\nhead(data, 10)\n\n# A tibble: 10 × 51\n   treatment control    ratio ratio2 ratio3    size size25 size50 size100 sizeno\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lb&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 2         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 3         1       0 1             0      0 3 [$10…      0      0       1      0\n 4         1       0 1             0      0 4 [Uns…      0      0       0      1\n 5         1       0 1             0      0 2 [$50…      0      1       0      0\n 6         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 7         1       0 1             0      0 1 [$25…      1      0       0      0\n 8         1       0 2             1      0 3 [$10…      0      0       1      0\n 9         1       0 2             1      0 4 [Uns…      0      0       0      1\n10         1       0 1             0      0 1 [$25…      1      0       0      0\n# … with 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;,\n#   askd3 &lt;dbl&gt;, ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;,\n#   stateresponsetminc &lt;dbl&gt;, perbush &lt;dbl&gt;, close25 &lt;dbl&gt;, red0 &lt;dbl&gt;, …\n\n\n\nsummary(data)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4044  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0094  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.7558  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.8728  \n Mean   :0.5956   Mean   :0.5102   Mean   :0.4887   Mean   :0.8196  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.9388  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866    \n     pblack         page18_39        ave_hh_sz     median_hhincome \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.0147   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.0366   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.0867   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.0909   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.9896   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036     NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\nThe dataset contains 50,083 observations, each representing a previous donor who received a fundraising solicitation as part of Karlan and List’s field experiment. The primary outcome variables include gave, indicating whether the person donated (mean = 0.0207), and amount, the dollar amount contributed (mean ≈ $0.92, conditional on giving). Most donors gave nothing, which is reflected by the median of $0.\nTreatment conditions are encoded with variables such as: - treatment: whether the respondent received a matching offer (≈ 66.7% did) - ratio: numeric indicator of the match ratio (mean ≈ 1.33) - ratio2 and ratio3: dummy variables for 2:1 and 3:1 match ratios - size25, size50, size100, sizeno: match threshold treatment groups\nSuggested donation levels are represented by ask, ask1, ask2, and ask3. Donor history is captured by variables such as freq (mean ≈ 8.04 prior donations), years (mean ≈ 6.10 years since first donation), and mrm2 (months since last donation, mean ≈ 13.0).\nDemographic and geographic controls include gender (female: ≈ 27.8%), relationship status (couple: ≈ 9.1%), political environment (red0, blue0), and census-level data like median_hhincome and pwhite.\nMost variables are binary indicators for treatment groups or demographics, and many are highly skewed, with means much lower than maximums.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo answer this, I conducted both t-tests and simple linear regressions comparing the following baseline variables across groups:\n\nmrm2: months since last donation\n\nfemale: gender indicator\n\nfreq: number of prior donations\n\nred0: whether the donor lives in a Bush-won (red) state\n\nThese variables help capture a mix of behavioral, demographic, and geographic features. For each one, I compute both the raw difference in means (via t-test) and the estimated treatment effect from a linear regression with treatment as the only predictor.\nThe table below summarizes the results:\n\nTTest_Diff: difference in means between groups\n\nReg_Estimate: coefficient on treatment from a linear model\n\np-values are reported for both approaches to assess statistical significance\n\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\n\ncovariates &lt;- c(\"mrm2\", \"female\", \"freq\", \"red0\")\n\nbalance_results &lt;- data.frame(\n  Variable = character(),\n  TTest_Diff = numeric(),\n  TTest_p = numeric(),\n  Reg_Estimate = numeric(),\n  Reg_SE = numeric(),\n  Reg_p = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (var in covariates) {\n  # T-test\n  ttest &lt;- t.test(data[[var]] ~ data$treatment)\n  \n  # Regression\n  reg &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  reg_coef &lt;- tidy(reg) %&gt;% filter(term == \"treatment\")\n  \n  balance_results &lt;- rbind(balance_results, data.frame(\n    Variable = var,\n    TTest_Diff = round(diff(ttest$estimate), 4),\n    TTest_p = round(ttest$p.value, 4),\n    Reg_Estimate = round(reg_coef$estimate, 4),\n    Reg_SE = round(reg_coef$std.error, 4),\n    Reg_p = round(reg_coef$p.value, 4)\n  ))\n}\n\n\nkable(balance_results, caption = \"Balance Test: Comparison of T-Test and Regression Results\")\n\n\nBalance Test: Comparison of T-Test and Regression Results\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nTTest_Diff\nTTest_p\nReg_Estimate\nReg_SE\nReg_p\n\n\n\n\nmean in group 1\nmrm2\n0.0137\n0.9049\n0.0137\n0.1145\n0.9049\n\n\nmean in group 11\nfemale\n-0.0075\n0.0795\n-0.0075\n0.0043\n0.0787\n\n\nmean in group 12\nfreq\n-0.0120\n0.9117\n-0.0120\n0.1080\n0.9117\n\n\nmean in group 13\nred0\n0.0087\n0.0605\n0.0087\n0.0047\n0.0608\n\n\n\n\n\nThe balance test results show that none of the selected variables differ significantly between the treatment and control groups. All p-values from both the t-tests and regressions are well above the conventional 0.05 threshold.\nThis is good news—it suggests that the randomization worked as intended. The treatment and control groups appear statistically similar in terms of donation history (mrm2, freq), gender (female), and political environment (red0).\nThese findings are consistent with Table 1 in Karlan and List (2007), which reports no significant differences across a wide range of covariates. By confirming that the two groups were well-balanced at baseline, we can more confidently attribute any differences in donation outcomes to the treatment itself."
  },
  {
    "objectID": "HW 1/hw1_questions.html#experimental-results",
    "href": "HW 1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ndonation_summary &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    prop_donated = mean(gave, na.rm = TRUE),\n    sample_size = n()\n  ) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\nggplot(donation_summary, aes(x = group, y = prop_donated, fill = group)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"Group\",\n    y = \"Donation Rate\"\n  ) +\n  ylim(0, max(donation_summary$prop_donated) * 1.1) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe barplot above shows the proportion of individuals who made a charitable contribution in each group. The treatment group, which received a letter including a matching donation offer, had a visibly higher donation rate than the control group, which received a standard fundraising letter. This visual difference suggests that the presence of a matching incentive may have positively influenced donors’ willingness to give.\n\nttest &lt;- t.test(gave ~ treatment, data = data)\n\nlm_gave &lt;- lm(gave ~ treatment, data = data)\nreg &lt;- tidy(lm_gave)\n\nresult_table &lt;- data.frame(\n  Group = c(\"Control\", \"Treatment\", \"T-test\", \"Regression\"),\n  DonationRate = round(c(\n    donation_summary$prop_donated[donation_summary$group == \"Control\"],\n    donation_summary$prop_donated[donation_summary$group == \"Treatment\"],\n    diff(donation_summary$prop_donated),\n    reg$estimate[2]\n  ), 4),\n  SampleSize = c(\n    donation_summary$sample_size[donation_summary$group == \"Control\"],\n    donation_summary$sample_size[donation_summary$group == \"Treatment\"],\n    NA,\n    NA\n  ),\n  p_value = c(\n    NA,\n    NA,\n    round(ttest$p.value, 4),\n    round(reg$p.value[2], 4)\n  )\n)\n\nkable(result_table, caption = \"Effect of Matching Offer on Donation Rate (gave)\")\n\n\nEffect of Matching Offer on Donation Rate (gave)\n\n\nGroup\nDonationRate\nSampleSize\np_value\n\n\n\n\nControl\n0.0179\n16687\nNA\n\n\nTreatment\n0.0220\n33396\nNA\n\n\nT-test\n0.0042\nNA\n0.0013\n\n\nRegression\n0.0042\nNA\n0.0019\n\n\n\n\n\nTo formally test whether the matching donation offer increased the likelihood of giving, I conducted both a t-test and a linear regression using gave as the outcome.\nThe treatment group had a donation rate of 2.20%, while the control group donated at 1.79%. Both the t-test and regression estimate a difference of about 0.42 percentage points, and the results are statistically significant with p-values below 0.01.\nTo further validate this finding, I also estimated a probit model with treatment as the sole predictor. While the probit coefficient itself is not directly interpretable, the average marginal effect (AME) reveals that the matching offer increases the donation probability by about 0.43 percentage points, consistent with the linear model.\nThese results closely align with Table 2A in Karlan and List (2007). Although the effect size is modest, the large sample size allows for precise estimation. From a behavioral perspective, it shows that even small nudges—like a matching offer—can meaningfully increase participation in charitable giving. Framing matters.\n\nlibrary(broom)\n\nprobit_model &lt;- glm(gave ~ treatment, family = binomial(link = \"probit\"), data = data)\n\nprobit_result &lt;- tidy(probit_model)\n\nprobit_result %&gt;%\n  filter(term == \"treatment\") %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 1 × 5\n  term      estimate std.error statistic p.value\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 treatment   0.0868    0.0279      3.11  0.0019\n\n\n\nlibrary(margins)\nmfx &lt;- margins(probit_model)\nsummary(mfx)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1045 0.0019 0.0016 0.0070\n\n\nTo further explore how the treatment influenced the likelihood of donating, I estimated a probit model using gave as the outcome and treatment as the only predictor.\nThe probit coefficient for the treatment group was 0.0868, and it was statistically significant at the 1% level (p = 0.0019). However, this number reflects a change on a latent index scale, which isn’t directly interpretable in terms of actual donation probabilities.\nTo get a more intuitive estimate, I used the margins package to calculate the average marginal effect (AME). The result shows that being assigned to the treatment group increases the probability of donating by about 0.43 percentage points (AME = 0.0043, p = 0.0019).\nThis finding is nearly identical to the linear regression result and matches what Karlan and List reported in Table 3, Column 1 of their original paper. Taken together, the t-test, linear model, and probit model all provide consistent evidence that matching offers significantly increase the likelihood of giving—even if the effect is relatively small. It’s a powerful reminder that framing and behavioral nudges can meaningfully influence donor decisions.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ntreat_only &lt;- data %&gt;% filter(treatment == 1)\nt_1_vs_2 &lt;- t.test(gave ~ (ratio == 2), data = treat_only)\nt_2_vs_3 &lt;- t.test(gave ~ (ratio == 3), data = treat_only %&gt;% filter(ratio %in% c(2, 3)))\nt_1_vs_3 &lt;- t.test(gave ~ (ratio == 3), data = treat_only %&gt;% filter(ratio %in% c(1, 3)))\n\nttest_table &lt;- bind_rows(\n  tidy(t_1_vs_2) %&gt;% mutate(Comparison = \"1:1 vs 2:1\"),\n  tidy(t_2_vs_3) %&gt;% mutate(Comparison = \"2:1 vs 3:1\"),\n  tidy(t_1_vs_3) %&gt;% mutate(Comparison = \"1:1 vs 3:1\")\n) %&gt;%\n  mutate(\n    Mean_Group_A = round(estimate1, 4),\n    Mean_Group_B = round(estimate2, 4),\n    Difference = round(estimate2 - estimate1, 4),\n    CI = paste0(\"[\", round(conf.low, 4), \", \", round(conf.high, 4), \"]\"),\n    p_value = round(p.value, 4)\n  ) %&gt;%\n  select(Comparison, Mean_Group_A, Mean_Group_B, Difference, CI, p_value)\n\nkable(ttest_table, caption = \"T-Test Comparison of Donation Rates Across Match Ratios\")\n\n\nT-Test Comparison of Donation Rates Across Match Ratios\n\n\n\n\n\n\n\n\n\n\nComparison\nMean_Group_A\nMean_Group_B\nDifference\nCI\np_value\n\n\n\n\n1:1 vs 2:1\n0.0217\n0.0226\n9e-04\n[-0.0043, 0.0025]\n0.6029\n\n\n2:1 vs 3:1\n0.0226\n0.0227\n1e-04\n[-0.004, 0.0038]\n0.9600\n\n\n1:1 vs 3:1\n0.0207\n0.0227\n2e-03\n[-0.0058, 0.0018]\n0.3101\n\n\n\n\n\nI compared the donation response rates among treatment groups that received different match ratios—1:1, 2:1, and 3:1—using pairwise t-tests.\nAcross all comparisons, the differences in response rates were extremely small (less than 0.2 percentage points) and statistically insignificant. For example, donors offered a 3:1 match were no more likely to give than those offered a 1:1 or 2:1 match.\nThese results reinforce one of the key insights from Karlan and List (2007): larger match ratios don’t necessarily increase donation rates. What seems to matter most is the existence of a match—regardless of how generous it is.\nFrom a behavioral standpoint, this suggests that donors are influenced by the idea of their gift being matched, but may not pay close attention to the specific match size. The framing effect—“your donation will be matched”—appears to be powerful enough on its own\n\nreg_ratio &lt;- lm(gave ~ ratio2 + ratio3, data = treat_only)\n\nlibrary(broom)\ntidy(reg_ratio, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0207    0.0014    14.9     0       0.018     0.0235\n2 ratio2        0.0019    0.002      0.958   0.338  -0.002     0.0057\n3 ratio3        0.002     0.002      1.01    0.313  -0.0019    0.0058\n\n\nTo further investigate whether the size of the match ratio influences donation behavior, I ran a linear regression using only the treatment group. In this model, ratio2 and ratio3 are indicator variables for the 2:1 and 3:1 match conditions. The 1:1 match group serves as the baseline, so the intercept represents the average donation rate for donors who received a 1:1 match.\nThe estimated donation rate under the 1:1 match was 2.07%, which is statistically significant (p &lt; 0.001). The 2:1 and 3:1 match groups had slightly higher rates—2.26% and 2.27%, respectively—but the differences (coefficients of 0.0019 and 0.0020) were not statistically significant (p = 0.3383 and 0.3133).\nThese findings are consistent with earlier t-test results and with the original study by Karlan and List (2007). They suggest that increasing the match ratio doesn’t meaningfully affect donation likelihood. Instead, it appears that the key behavioral lever is simply the presence of a match offer, not how generous the match is.\n\nmean_by_ratio &lt;- treat_only %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\n\nmean_by_ratio\n\n# A tibble: 3 × 2\n      ratio response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1         1        0.0207\n2         2        0.0226\n3         3        0.0227\n\n\n\ndiff_2_1 &lt;- mean_by_ratio$response_rate[mean_by_ratio$ratio == 2] - mean_by_ratio$response_rate[mean_by_ratio$ratio == 1]\ndiff_3_2 &lt;- mean_by_ratio$response_rate[mean_by_ratio$ratio == 3] - mean_by_ratio$response_rate[mean_by_ratio$ratio == 2]\n\ndiff_2_1\n\n[1] 0.001884251\n\ndiff_3_2\n\n[1] 0.000100024\n\n\n\ncoef_table &lt;- tidy(reg_ratio)\n\nbaseline &lt;- coef_table$estimate[coef_table$term == \"(Intercept)\"]\ncoef_2_1 &lt;- coef_table$estimate[coef_table$term == \"ratio2\"]\ncoef_3_1 &lt;- coef_table$estimate[coef_table$term == \"ratio3\"]\n\nfitted_2 &lt;- baseline + coef_2_1\nfitted_3 &lt;- baseline + coef_3_1\n\ndiff_2_1_fitted &lt;- coef_2_1\ndiff_3_2_fitted &lt;- coef_3_1 - coef_2_1\n\ndiff_2_1_fitted\n\n[1] 0.001884251\n\ndiff_3_2_fitted\n\n[1] 0.000100024\n\n\nTo further assess the effectiveness of different match sizes, I compared donation rates between match ratios using two approaches:\n(1) directly from the observed data, and\n(2) indirectly using fitted values from the regression model.\nThe difference in donation rates between the 1:1 and 2:1 match groups was approximately 0.00188, or 0.19 percentage points. Between the 2:1 and 3:1 match groups, the difference was only 0.00010, or 0.01 percentage points. These differences were exactly confirmed using the regression coefficients, which adds confidence to the consistency of the findings.\nWhile the differences are numerically real, both are very small in magnitude and, as shown earlier, not statistically significant. This reinforces the conclusion that although a matching offer increases donations compared to no match, increasing the match ratio beyond 1:1 does not meaningfully boost response rates.\nThe key takeaway is clear: the existence of a match matters more than its generosity. For fundraisers, this insight suggests that match framing can be powerful—even without increasing the cost of the match itself.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nt_test_amount &lt;- t.test(amount ~ treatment, data = data)\n\nreg_amount &lt;- lm(amount ~ treatment, data = data)\nlibrary(broom)\ntidy(reg_amount, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1   0        0.681      0.945\n2 treatment      0.154    0.0826      1.86  0.0628  -0.0082     0.315\n\n\nI began by examining the effect of the treatment on total donation amounts using a simple linear regression. This analysis includes all individuals, regardless of whether they actually donated.\nThe results show that participants in the treatment group gave, on average, $0.15 more than those in the control group (estimate = 0.1536). However, this difference is not statistically significant at the 5% level (p = 0.0628).\nThis suggests that while the treatment group may have contributed slightly more overall, the difference could be driven primarily by a higher number of people donating, not by larger gifts among donors. In other words, this part of the analysis reflects the extensive margin (whether someone donates), not the intensive margin (how much they give once they do).\n\ndonors_only &lt;- data %&gt;% filter(gave == 1)\n\nreg_amount_conditional &lt;- lm(amount ~ treatment, data = donors_only)\n\nlibrary(broom)\ntidy(reg_amount_conditional, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8     0        40.8      50.3 \n2 treatment      -1.67      2.87    -0.581   0.562    -7.30      3.97\n\n\nNext, I restricted the analysis to only those individuals who actually made a donation.\nAmong donors, the average donation amount in the control group was $45.54, while donors in the treatment group gave $1.67 less on average. This difference is again not statistically significant (p = 0.5615).\nThese results suggest that once someone decides to donate, the matching offer has no clear effect on how much they give. Combined with the earlier finding that matching increases the likelihood of giving, this implies that the treatment works mainly by encouraging people to donate, not by changing donation size.\n\ndonors_only &lt;- data %&gt;%\n  filter(gave == 1) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\nmean_amounts &lt;- donors_only %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_amt = mean(amount, na.rm = TRUE))\n\nggplot(donors_only, aes(x = amount)) +\n  geom_histogram(bins = 40, fill = \"skyblue\", color = \"white\") +\n  geom_vline(data = mean_amounts,\n             aes(xintercept = mean_amt),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ group) +\n  labs(\n    title = \"Distribution of Donation Amounts by Group (Among Donors Only)\",\n    x = \"Donation Amount ($)\",\n    y = \"Number of Donors\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of donation amounts for each group.\nAt first glance, the two distributions appear almost identical—both are centered around $45–$50 and have a similar shape. The dashed red lines represent the group averages: approximately $45.5 for the control group and slightly lower for the treatment group.\nSurprisingly, those in the treatment group actually gave a bit less than the control group. But again, this difference is not statistically meaningful, and most likely due to random variation.\nSo what can we take away from this? While a matching offer can successfully nudge people to donate, it doesn’t influence how generous they are once they’ve made that decision. The effect seems to operate on whether someone acts, not how much they give—an important insight for designing effective fundraising strategies."
  },
  {
    "objectID": "HW 1/hw1_questions.html#simulation-experiment",
    "href": "HW 1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(42)\n\ncontrol &lt;- rbinom(10000, size = 1, prob = 0.018)\ntreatment &lt;- rbinom(10000, size = 1, prob = 0.022)\n\ndiffs &lt;- treatment - control\n\ncumulative_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\nlibrary(ggplot2)\nplot_data &lt;- data.frame(\n  Draw = 1:10000,\n  CumulativeAverage = cumulative_avg\n)\n\nggplot(plot_data, aes(x = Draw, y = CumulativeAverage)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_hline(yintercept = 0.004, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Cumulative Average of Treatment - Control Donations (Simulated)\",\n    subtitle = \"Demonstrating the Law of Large Numbers\",\n    x = \"Number of Simulations\",\n    y = \"Cumulative Average of Differences\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis simulation clearly demonstrates the Law of Large Numbers in action.\nAs we simulate more and more observations, the cumulative average difference in donation rates between the treatment and control groups steadily converges to the true expected value of 0.004. Early on, we see a lot of noise—the line bounces around as randomness dominates small sample sizes. But as the number of simulations increases, the fluctuations smooth out and the line settles near the true value.\nThis convergence gives us confidence in the reliability of our estimates. The more data we collect, the closer we get to the underlying truth—even when individual observations are binary and noisy, like whether someone donates.\n\n\nCentral Limit Theorem\n\nset.seed(123)\n\nsample_sizes &lt;- c(50, 200, 500, 1000)\nnum_simulations &lt;- 1000\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ tibble  3.1.5     ✔ purrr   1.0.2\n✔ tidyr   1.1.4     ✔ stringr 1.4.0\n✔ readr   2.0.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nsim_results &lt;- lapply(sample_sizes, function(n) {\n  diffs &lt;- replicate(num_simulations, {\n    treat &lt;- rbinom(n, size = 1, prob = 0.022)\n    control &lt;- rbinom(n, size = 1, prob = 0.018)\n    mean(treat) - mean(control)\n  })\n  tibble(\n    diff = diffs,\n    sample_size = n\n  )\n}) %&gt;% bind_rows()\n\n\nggplot(sim_results, aes(x = diff)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  facet_wrap(~ sample_size, scales = \"free\", ncol = 2) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Distribution of Simulated Mean Differences\",\n    subtitle = \"Across Varying Sample Sizes (Central Limit Theorem)\",\n    x = \"Average Treatment - Control\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe histograms below illustrate the Central Limit Theorem (CLT) in action.\nFor each sample size—50, 200, 500, and 1000—I repeatedly simulated the average difference in donation rates between a treatment group (with donation probability = 0.022) and a control group (0.018). Each histogram shows the distribution of these simulated mean differences.\nAs the sample size increases, the distribution becomes smoother, more symmetric, and narrower. With only 50 observations, the results are jagged and spread out—random chance can easily mask the treatment effect. But by the time we reach 1000 observations, the distribution clusters tightly around the true value (≈ 0.004), and resembles a classic bell curve.\nThe red dashed line at zero represents the “no effect” case. In small samples, it’s often close to the center of the distribution. But in large samples, zero moves to the tail, meaning the evidence increasingly rules out no effect.\nThis is exactly why the CLT is so powerful in applied work: with large enough samples, our estimates become well-behaved, normally distributed, and centered near the truth, even when the raw data is binary and highly skewed."
  },
  {
    "objectID": "HW 1/hw1_questions.html#conclusion",
    "href": "HW 1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "HW 1/hw1_questions.html#conclusion-1",
    "href": "HW 1/hw1_questions.html#conclusion-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis replication of Karlan and List (2007) confirmed several key insights about how simple behavioral cues can shape charitable giving.\nI was especially struck by how just the presence of a match—regardless of its size—was enough to increase donation rates. Whether the offer was 1:1 or 3:1, donors were equally likely to give. What mattered most was the framing: knowing their donation would be matched seemed to motivate action.\nInterestingly, once people decided to donate, the matching offer didn’t affect how much they gave. This suggests that the treatment primarily affects the decision to give (the extensive margin), rather than the donation amount (the intensive margin).\nAs someone studying data science and business analytics, this project reminded me that small psychological shifts can lead to measurable real-world effects. For fundraisers or policymakers, it’s a strong case for thoughtful design and messaging.\nSometimes, it’s not about offering more. It’s about offering it more effectively."
  },
  {
    "objectID": "Blog/Project 1/index.html",
    "href": "Blog/Project 1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo briefly summarize the design: 50,083 past donors were randomly assigned to one of two groups—a control group (which received a standard appeal) and a treatment group (which received a matching offer). Within the treatment group, donors were further randomly assigned to one of several specific conditions: - Match ratios: 1:1, 2:1, or 3:1 (e.g., a $50 donation with a 2:1 match results in $150 total) - Match thresholds: limits of $25,000, $50,000, $100,000, or no cap - Suggested donation amounts: equal to, 1.25×, or 1.5× the donor’s previous maximum gift\nThe core message and layout of each letter remained the same—only the matching and suggestion elements varied. The goal was to test how these subtle framing differences affected both whether people chose to donate and how much they gave.\nIn this project, I replicate their study using the original dataset, examining both the balance across groups and the causal impact of different treatments on giving behavior."
  },
  {
    "objectID": "Blog/Project 1/index.html#introduction",
    "href": "Blog/Project 1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo briefly summarize the design: 50,083 past donors were randomly assigned to one of two groups—a control group (which received a standard appeal) and a treatment group (which received a matching offer). Within the treatment group, donors were further randomly assigned to one of several specific conditions: - Match ratios: 1:1, 2:1, or 3:1 (e.g., a $50 donation with a 2:1 match results in $150 total) - Match thresholds: limits of $25,000, $50,000, $100,000, or no cap - Suggested donation amounts: equal to, 1.25×, or 1.5× the donor’s previous maximum gift\nThe core message and layout of each letter remained the same—only the matching and suggestion elements varied. The goal was to test how these subtle framing differences affected both whether people chose to donate and how much they gave.\nIn this project, I replicate their study using the original dataset, examining both the balance across groups and the causal impact of different treatments on giving behavior."
  },
  {
    "objectID": "Blog/Project 1/index.html#data",
    "href": "Blog/Project 1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nlibrary(haven)  \nlibrary(dplyr)    \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  \n\nThe dataset includes 50,083 observations and 51 variables. Each row corresponds to a past donor who was randomly assigned to receive one of several fundraising letters as part of Karlan and List’s experiment.\nBelow is a preview of the first 10 rows:\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\nhead(data, 10)\n\n# A tibble: 10 × 51\n   treatment control    ratio ratio2 ratio3    size size25 size50 size100 sizeno\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lb&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 2         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 3         1       0 1             0      0 3 [$10…      0      0       1      0\n 4         1       0 1             0      0 4 [Uns…      0      0       0      1\n 5         1       0 1             0      0 2 [$50…      0      1       0      0\n 6         0       1 0 [Cont…      0      0 0 [Con…      0      0       0      0\n 7         1       0 1             0      0 1 [$25…      1      0       0      0\n 8         1       0 2             1      0 3 [$10…      0      0       1      0\n 9         1       0 2             1      0 4 [Uns…      0      0       0      1\n10         1       0 1             0      0 1 [$25…      1      0       0      0\n# … with 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;,\n#   askd3 &lt;dbl&gt;, ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;,\n#   stateresponsetminc &lt;dbl&gt;, perbush &lt;dbl&gt;, close25 &lt;dbl&gt;, red0 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\n\n\n\nsummary(data)\n\n   treatment         control           ratio           ratio2      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.6668   Mean   :0.3332   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     ratio3            size           size25           size50      \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :2.000   Median :0.0000   Median :0.0000  \n Mean   :0.2222   Mean   :1.667   Mean   :0.1667   Mean   :0.1666  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :4.000   Max.   :1.0000   Max.   :1.0000  \n                                                                   \n    size100           sizeno            ask            askd1       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.0000  \n Mean   :0.1667   Mean   :0.1667   Mean   :1.334   Mean   :0.2223  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:2.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :3.000   Max.   :1.0000  \n                                                                   \n     askd2            askd3             ask1             ask2        \n Min.   :0.0000   Min.   :0.0000   Min.   :  25.0   Min.   :  35.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  35.0   1st Qu.:  45.00  \n Median :0.0000   Median :0.0000   Median :  45.0   Median :  60.00  \n Mean   :0.2223   Mean   :0.2222   Mean   :  71.5   Mean   :  91.79  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:  65.0   3rd Qu.:  85.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1500.0   Max.   :1875.00  \n                                                                     \n      ask3          amount              gave          amountchange       \n Min.   :  50   Min.   :  0.0000   Min.   :0.00000   Min.   :-200412.12  \n 1st Qu.:  55   1st Qu.:  0.0000   1st Qu.:0.00000   1st Qu.:    -50.00  \n Median :  70   Median :  0.0000   Median :0.00000   Median :    -30.00  \n Mean   : 111   Mean   :  0.9157   Mean   :0.02065   Mean   :    -52.67  \n 3rd Qu.: 100   3rd Qu.:  0.0000   3rd Qu.:0.00000   3rd Qu.:    -25.00  \n Max.   :2250   Max.   :400.0000   Max.   :1.00000   Max.   :    275.00  \n                                                                         \n      hpa             ltmedmra           freq             years       \n Min.   :   0.00   Min.   :0.0000   Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  30.00   1st Qu.:0.0000   1st Qu.:  2.000   1st Qu.: 2.000  \n Median :  45.00   Median :0.0000   Median :  4.000   Median : 5.000  \n Mean   :  59.38   Mean   :0.4937   Mean   :  8.039   Mean   : 6.098  \n 3rd Qu.:  60.00   3rd Qu.:1.0000   3rd Qu.: 10.000   3rd Qu.: 9.000  \n Max.   :1000.00   Max.   :1.0000   Max.   :218.000   Max.   :95.000  \n                                                      NA's   :1       \n     year5             mrm2           dormant           female      \n Min.   :0.0000   Min.   :  0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:  4.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :  8.00   Median :1.0000   Median :0.0000  \n Mean   :0.5088   Mean   : 13.01   Mean   :0.5235   Mean   :0.2777  \n 3rd Qu.:1.0000   3rd Qu.: 19.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :168.00   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :1                         NA's   :1111    \n     couple         state50one            nonlit          cases    \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:1.000   1st Qu.:1.0  \n Median :0.0000   Median :0.0000000   Median :3.000   Median :1.0  \n Mean   :0.0919   Mean   :0.0009983   Mean   :2.474   Mean   :1.5  \n 3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:4.000   3rd Qu.:2.0  \n Max.   :1.0000   Max.   :1.0000000   Max.   :6.000   Max.   :4.0  \n NA's   :1148                         NA's   :452     NA's   :452  \n    statecnt         stateresponse     stateresponset    stateresponsec   \n Min.   : 0.001995   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 1.833234   1st Qu.:0.01816   1st Qu.:0.01849   1st Qu.:0.01286  \n Median : 3.538799   Median :0.01971   Median :0.02170   Median :0.01988  \n Mean   : 5.998820   Mean   :0.02063   Mean   :0.02199   Mean   :0.01772  \n 3rd Qu.: 9.607021   3rd Qu.:0.02305   3rd Qu.:0.02470   3rd Qu.:0.02081  \n Max.   :17.368841   Max.   :0.07692   Max.   :0.11111   Max.   :0.05263  \n                                                         NA's   :3        \n stateresponsetminc     perbush           close25            red0       \n Min.   :-0.047619   Min.   :0.09091   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.001388   1st Qu.:0.44444   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 0.001779   Median :0.48485   Median :0.0000   Median :0.0000  \n Mean   : 0.004273   Mean   :0.48794   Mean   :0.1857   Mean   :0.4044  \n 3rd Qu.: 0.010545   3rd Qu.:0.52525   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   : 0.111111   Max.   :0.73196   Max.   :1.0000   Max.   :1.0000  \n NA's   :3           NA's   :35        NA's   :35       NA's   :35      \n     blue0            redcty          bluecty           pwhite      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0094  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.7558  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.8728  \n Mean   :0.5956   Mean   :0.5102   Mean   :0.4887   Mean   :0.8196  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.9388  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :35       NA's   :105      NA's   :105      NA's   :1866    \n     pblack         page18_39        ave_hh_sz     median_hhincome \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :  5000  \n 1st Qu.:0.0147   1st Qu.:0.2583   1st Qu.:2.210   1st Qu.: 39181  \n Median :0.0366   Median :0.3055   Median :2.440   Median : 50673  \n Mean   :0.0867   Mean   :0.3217   Mean   :2.429   Mean   : 54816  \n 3rd Qu.:0.0909   3rd Qu.:0.3691   3rd Qu.:2.660   3rd Qu.: 66005  \n Max.   :0.9896   Max.   :0.9975   Max.   :5.270   Max.   :200001  \n NA's   :2036     NA's   :1866     NA's   :1862    NA's   :1874    \n     powner        psch_atlstba    pop_propurban   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.5602   1st Qu.:0.2356   1st Qu.:0.8849  \n Median :0.7123   Median :0.3737   Median :1.0000  \n Mean   :0.6694   Mean   :0.3917   Mean   :0.8720  \n 3rd Qu.:0.8168   3rd Qu.:0.5300   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1869     NA's   :1868     NA's   :1866    \n\n\n\n\n\nThe dataset contains 50,083 observations, each representing a previous donor who received a fundraising solicitation as part of Karlan and List’s field experiment. The primary outcome variables include gave, indicating whether the person donated (mean = 0.0207), and amount, the dollar amount contributed (mean ≈ $0.92, conditional on giving). Most donors gave nothing, which is reflected by the median of $0.\nTreatment conditions are encoded with variables such as: - treatment: whether the respondent received a matching offer (≈ 66.7% did) - ratio: numeric indicator of the match ratio (mean ≈ 1.33) - ratio2 and ratio3: dummy variables for 2:1 and 3:1 match ratios - size25, size50, size100, sizeno: match threshold treatment groups\nSuggested donation levels are represented by ask, ask1, ask2, and ask3. Donor history is captured by variables such as freq (mean ≈ 8.04 prior donations), years (mean ≈ 6.10 years since first donation), and mrm2 (months since last donation, mean ≈ 13.0).\nDemographic and geographic controls include gender (female: ≈ 27.8%), relationship status (couple: ≈ 9.1%), political environment (red0, blue0), and census-level data like median_hhincome and pwhite.\nMost variables are binary indicators for treatment groups or demographics, and many are highly skewed, with means much lower than maximums.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo answer this, I conducted both t-tests and simple linear regressions comparing the following baseline variables across groups:\n\nmrm2: months since last donation\n\nfemale: gender indicator\n\nfreq: number of prior donations\n\nred0: whether the donor lives in a Bush-won (red) state\n\nThese variables help capture a mix of behavioral, demographic, and geographic features. For each one, I compute both the raw difference in means (via t-test) and the estimated treatment effect from a linear regression with treatment as the only predictor.\nThe table below summarizes the results:\n\nTTest_Diff: difference in means between groups\n\nReg_Estimate: coefficient on treatment from a linear model\n\np-values are reported for both approaches to assess statistical significance\n\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\n\ncovariates &lt;- c(\"mrm2\", \"female\", \"freq\", \"red0\")\n\nbalance_results &lt;- data.frame(\n  Variable = character(),\n  TTest_Diff = numeric(),\n  TTest_p = numeric(),\n  Reg_Estimate = numeric(),\n  Reg_SE = numeric(),\n  Reg_p = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (var in covariates) {\n  # T-test\n  ttest &lt;- t.test(data[[var]] ~ data$treatment)\n  \n  # Regression\n  reg &lt;- lm(as.formula(paste(var, \"~ treatment\")), data = data)\n  reg_coef &lt;- tidy(reg) %&gt;% filter(term == \"treatment\")\n  \n  balance_results &lt;- rbind(balance_results, data.frame(\n    Variable = var,\n    TTest_Diff = round(diff(ttest$estimate), 4),\n    TTest_p = round(ttest$p.value, 4),\n    Reg_Estimate = round(reg_coef$estimate, 4),\n    Reg_SE = round(reg_coef$std.error, 4),\n    Reg_p = round(reg_coef$p.value, 4)\n  ))\n}\n\n\nkable(balance_results, caption = \"Balance Test: Comparison of T-Test and Regression Results\")\n\n\nBalance Test: Comparison of T-Test and Regression Results\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nTTest_Diff\nTTest_p\nReg_Estimate\nReg_SE\nReg_p\n\n\n\n\nmean in group 1\nmrm2\n0.0137\n0.9049\n0.0137\n0.1145\n0.9049\n\n\nmean in group 11\nfemale\n-0.0075\n0.0795\n-0.0075\n0.0043\n0.0787\n\n\nmean in group 12\nfreq\n-0.0120\n0.9117\n-0.0120\n0.1080\n0.9117\n\n\nmean in group 13\nred0\n0.0087\n0.0605\n0.0087\n0.0047\n0.0608\n\n\n\n\n\nThe balance test results show that none of the selected variables differ significantly between the treatment and control groups. All p-values from both the t-tests and regressions are well above the conventional 0.05 threshold.\nThis is good news—it suggests that the randomization worked as intended. The treatment and control groups appear statistically similar in terms of donation history (mrm2, freq), gender (female), and political environment (red0).\nThese findings are consistent with Table 1 in Karlan and List (2007), which reports no significant differences across a wide range of covariates. By confirming that the two groups were well-balanced at baseline, we can more confidently attribute any differences in donation outcomes to the treatment itself."
  },
  {
    "objectID": "Blog/Project 1/index.html#experimental-results",
    "href": "Blog/Project 1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ndonation_summary &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    prop_donated = mean(gave, na.rm = TRUE),\n    sample_size = n()\n  ) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\nggplot(donation_summary, aes(x = group, y = prop_donated, fill = group)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"Group\",\n    y = \"Donation Rate\"\n  ) +\n  ylim(0, max(donation_summary$prop_donated) * 1.1) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe barplot above shows the proportion of individuals who made a charitable contribution in each group. The treatment group, which received a letter including a matching donation offer, had a visibly higher donation rate than the control group, which received a standard fundraising letter. This visual difference suggests that the presence of a matching incentive may have positively influenced donors’ willingness to give.\n\nttest &lt;- t.test(gave ~ treatment, data = data)\n\nlm_gave &lt;- lm(gave ~ treatment, data = data)\nreg &lt;- tidy(lm_gave)\n\nresult_table &lt;- data.frame(\n  Group = c(\"Control\", \"Treatment\", \"T-test\", \"Regression\"),\n  DonationRate = round(c(\n    donation_summary$prop_donated[donation_summary$group == \"Control\"],\n    donation_summary$prop_donated[donation_summary$group == \"Treatment\"],\n    diff(donation_summary$prop_donated),\n    reg$estimate[2]\n  ), 4),\n  SampleSize = c(\n    donation_summary$sample_size[donation_summary$group == \"Control\"],\n    donation_summary$sample_size[donation_summary$group == \"Treatment\"],\n    NA,\n    NA\n  ),\n  p_value = c(\n    NA,\n    NA,\n    round(ttest$p.value, 4),\n    round(reg$p.value[2], 4)\n  )\n)\n\nkable(result_table, caption = \"Effect of Matching Offer on Donation Rate (gave)\")\n\n\nEffect of Matching Offer on Donation Rate (gave)\n\n\nGroup\nDonationRate\nSampleSize\np_value\n\n\n\n\nControl\n0.0179\n16687\nNA\n\n\nTreatment\n0.0220\n33396\nNA\n\n\nT-test\n0.0042\nNA\n0.0013\n\n\nRegression\n0.0042\nNA\n0.0019\n\n\n\n\n\nTo formally test whether the matching donation offer increased the likelihood of giving, I conducted both a t-test and a linear regression using gave as the outcome.\nThe treatment group had a donation rate of 2.20%, while the control group donated at 1.79%. Both the t-test and regression estimate a difference of about 0.42 percentage points, and the results are statistically significant with p-values below 0.01.\nTo further validate this finding, I also estimated a probit model with treatment as the sole predictor. While the probit coefficient itself is not directly interpretable, the average marginal effect (AME) reveals that the matching offer increases the donation probability by about 0.43 percentage points, consistent with the linear model.\nThese results closely align with Table 2A in Karlan and List (2007). Although the effect size is modest, the large sample size allows for precise estimation. From a behavioral perspective, it shows that even small nudges—like a matching offer—can meaningfully increase participation in charitable giving. Framing matters.\n\nlibrary(broom)\n\nprobit_model &lt;- glm(gave ~ treatment, family = binomial(link = \"probit\"), data = data)\n\nprobit_result &lt;- tidy(probit_model)\n\nprobit_result %&gt;%\n  filter(term == \"treatment\") %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 1 × 5\n  term      estimate std.error statistic p.value\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 treatment   0.0868    0.0279      3.11  0.0019\n\n\n\nlibrary(margins)\nmfx &lt;- margins(probit_model)\nsummary(mfx)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1045 0.0019 0.0016 0.0070\n\n\nTo further explore how the treatment influenced the likelihood of donating, I estimated a probit model using gave as the outcome and treatment as the only predictor.\nThe probit coefficient for the treatment group was 0.0868, and it was statistically significant at the 1% level (p = 0.0019). However, this number reflects a change on a latent index scale, which isn’t directly interpretable in terms of actual donation probabilities.\nTo get a more intuitive estimate, I used the margins package to calculate the average marginal effect (AME). The result shows that being assigned to the treatment group increases the probability of donating by about 0.43 percentage points (AME = 0.0043, p = 0.0019).\nThis finding is nearly identical to the linear regression result and matches what Karlan and List reported in Table 3, Column 1 of their original paper. Taken together, the t-test, linear model, and probit model all provide consistent evidence that matching offers significantly increase the likelihood of giving—even if the effect is relatively small. It’s a powerful reminder that framing and behavioral nudges can meaningfully influence donor decisions.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ntreat_only &lt;- data %&gt;% filter(treatment == 1)\nt_1_vs_2 &lt;- t.test(gave ~ (ratio == 2), data = treat_only)\nt_2_vs_3 &lt;- t.test(gave ~ (ratio == 3), data = treat_only %&gt;% filter(ratio %in% c(2, 3)))\nt_1_vs_3 &lt;- t.test(gave ~ (ratio == 3), data = treat_only %&gt;% filter(ratio %in% c(1, 3)))\n\nttest_table &lt;- bind_rows(\n  tidy(t_1_vs_2) %&gt;% mutate(Comparison = \"1:1 vs 2:1\"),\n  tidy(t_2_vs_3) %&gt;% mutate(Comparison = \"2:1 vs 3:1\"),\n  tidy(t_1_vs_3) %&gt;% mutate(Comparison = \"1:1 vs 3:1\")\n) %&gt;%\n  mutate(\n    Mean_Group_A = round(estimate1, 4),\n    Mean_Group_B = round(estimate2, 4),\n    Difference = round(estimate2 - estimate1, 4),\n    CI = paste0(\"[\", round(conf.low, 4), \", \", round(conf.high, 4), \"]\"),\n    p_value = round(p.value, 4)\n  ) %&gt;%\n  select(Comparison, Mean_Group_A, Mean_Group_B, Difference, CI, p_value)\n\nkable(ttest_table, caption = \"T-Test Comparison of Donation Rates Across Match Ratios\")\n\n\nT-Test Comparison of Donation Rates Across Match Ratios\n\n\n\n\n\n\n\n\n\n\nComparison\nMean_Group_A\nMean_Group_B\nDifference\nCI\np_value\n\n\n\n\n1:1 vs 2:1\n0.0217\n0.0226\n9e-04\n[-0.0043, 0.0025]\n0.6029\n\n\n2:1 vs 3:1\n0.0226\n0.0227\n1e-04\n[-0.004, 0.0038]\n0.9600\n\n\n1:1 vs 3:1\n0.0207\n0.0227\n2e-03\n[-0.0058, 0.0018]\n0.3101\n\n\n\n\n\nI compared the donation response rates among treatment groups that received different match ratios—1:1, 2:1, and 3:1—using pairwise t-tests.\nAcross all comparisons, the differences in response rates were extremely small (less than 0.2 percentage points) and statistically insignificant. For example, donors offered a 3:1 match were no more likely to give than those offered a 1:1 or 2:1 match.\nThese results reinforce one of the key insights from Karlan and List (2007): larger match ratios don’t necessarily increase donation rates. What seems to matter most is the existence of a match—regardless of how generous it is.\nFrom a behavioral standpoint, this suggests that donors are influenced by the idea of their gift being matched, but may not pay close attention to the specific match size. The framing effect—“your donation will be matched”—appears to be powerful enough on its own\n\nreg_ratio &lt;- lm(gave ~ ratio2 + ratio3, data = treat_only)\n\nlibrary(broom)\ntidy(reg_ratio, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0207    0.0014    14.9     0       0.018     0.0235\n2 ratio2        0.0019    0.002      0.958   0.338  -0.002     0.0057\n3 ratio3        0.002     0.002      1.01    0.313  -0.0019    0.0058\n\n\nTo further investigate whether the size of the match ratio influences donation behavior, I ran a linear regression using only the treatment group. In this model, ratio2 and ratio3 are indicator variables for the 2:1 and 3:1 match conditions. The 1:1 match group serves as the baseline, so the intercept represents the average donation rate for donors who received a 1:1 match.\nThe estimated donation rate under the 1:1 match was 2.07%, which is statistically significant (p &lt; 0.001). The 2:1 and 3:1 match groups had slightly higher rates—2.26% and 2.27%, respectively—but the differences (coefficients of 0.0019 and 0.0020) were not statistically significant (p = 0.3383 and 0.3133).\nThese findings are consistent with earlier t-test results and with the original study by Karlan and List (2007). They suggest that increasing the match ratio doesn’t meaningfully affect donation likelihood. Instead, it appears that the key behavioral lever is simply the presence of a match offer, not how generous the match is.\n\nmean_by_ratio &lt;- treat_only %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE))\n\nmean_by_ratio\n\n# A tibble: 3 × 2\n      ratio response_rate\n  &lt;dbl+lbl&gt;         &lt;dbl&gt;\n1         1        0.0207\n2         2        0.0226\n3         3        0.0227\n\n\n\ndiff_2_1 &lt;- mean_by_ratio$response_rate[mean_by_ratio$ratio == 2] - mean_by_ratio$response_rate[mean_by_ratio$ratio == 1]\ndiff_3_2 &lt;- mean_by_ratio$response_rate[mean_by_ratio$ratio == 3] - mean_by_ratio$response_rate[mean_by_ratio$ratio == 2]\n\ndiff_2_1\n\n[1] 0.001884251\n\ndiff_3_2\n\n[1] 0.000100024\n\n\n\ncoef_table &lt;- tidy(reg_ratio)\n\nbaseline &lt;- coef_table$estimate[coef_table$term == \"(Intercept)\"]\ncoef_2_1 &lt;- coef_table$estimate[coef_table$term == \"ratio2\"]\ncoef_3_1 &lt;- coef_table$estimate[coef_table$term == \"ratio3\"]\n\nfitted_2 &lt;- baseline + coef_2_1\nfitted_3 &lt;- baseline + coef_3_1\n\ndiff_2_1_fitted &lt;- coef_2_1\ndiff_3_2_fitted &lt;- coef_3_1 - coef_2_1\n\ndiff_2_1_fitted\n\n[1] 0.001884251\n\ndiff_3_2_fitted\n\n[1] 0.000100024\n\n\nTo further assess the effectiveness of different match sizes, I compared donation rates between match ratios using two approaches:\n(1) directly from the observed data, and\n(2) indirectly using fitted values from the regression model.\nThe difference in donation rates between the 1:1 and 2:1 match groups was approximately 0.00188, or 0.19 percentage points. Between the 2:1 and 3:1 match groups, the difference was only 0.00010, or 0.01 percentage points. These differences were exactly confirmed using the regression coefficients, which adds confidence to the consistency of the findings.\nWhile the differences are numerically real, both are very small in magnitude and, as shown earlier, not statistically significant. This reinforces the conclusion that although a matching offer increases donations compared to no match, increasing the match ratio beyond 1:1 does not meaningfully boost response rates.\nThe key takeaway is clear: the existence of a match matters more than its generosity. For fundraisers, this insight suggests that match framing can be powerful—even without increasing the cost of the match itself.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nt_test_amount &lt;- t.test(amount ~ treatment, data = data)\n\nreg_amount &lt;- lm(amount ~ treatment, data = data)\nlibrary(broom)\ntidy(reg_amount, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1   0        0.681      0.945\n2 treatment      0.154    0.0826      1.86  0.0628  -0.0082     0.315\n\n\nI began by examining the effect of the treatment on total donation amounts using a simple linear regression. This analysis includes all individuals, regardless of whether they actually donated.\nThe results show that participants in the treatment group gave, on average, $0.15 more than those in the control group (estimate = 0.1536). However, this difference is not statistically significant at the 5% level (p = 0.0628).\nThis suggests that while the treatment group may have contributed slightly more overall, the difference could be driven primarily by a higher number of people donating, not by larger gifts among donors. In other words, this part of the analysis reflects the extensive margin (whether someone donates), not the intensive margin (how much they give once they do).\n\ndonors_only &lt;- data %&gt;% filter(gave == 1)\n\nreg_amount_conditional &lt;- lm(amount ~ treatment, data = donors_only)\n\nlibrary(broom)\ntidy(reg_amount_conditional, conf.int = TRUE) %&gt;%\n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8     0        40.8      50.3 \n2 treatment      -1.67      2.87    -0.581   0.562    -7.30      3.97\n\n\nNext, I restricted the analysis to only those individuals who actually made a donation.\nAmong donors, the average donation amount in the control group was $45.54, while donors in the treatment group gave $1.67 less on average. This difference is again not statistically significant (p = 0.5615).\nThese results suggest that once someone decides to donate, the matching offer has no clear effect on how much they give. Combined with the earlier finding that matching increases the likelihood of giving, this implies that the treatment works mainly by encouraging people to donate, not by changing donation size.\n\ndonors_only &lt;- data %&gt;%\n  filter(gave == 1) %&gt;%\n  mutate(group = ifelse(treatment == 1, \"Treatment\", \"Control\"))\n\nmean_amounts &lt;- donors_only %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_amt = mean(amount, na.rm = TRUE))\n\nggplot(donors_only, aes(x = amount)) +\n  geom_histogram(bins = 40, fill = \"skyblue\", color = \"white\") +\n  geom_vline(data = mean_amounts,\n             aes(xintercept = mean_amt),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ group) +\n  labs(\n    title = \"Distribution of Donation Amounts by Group (Among Donors Only)\",\n    x = \"Donation Amount ($)\",\n    y = \"Number of Donors\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of donation amounts for each group.\nAt first glance, the two distributions appear almost identical—both are centered around $45–$50 and have a similar shape. The dashed red lines represent the group averages: approximately $45.5 for the control group and slightly lower for the treatment group.\nSurprisingly, those in the treatment group actually gave a bit less than the control group. But again, this difference is not statistically meaningful, and most likely due to random variation.\nSo what can we take away from this? While a matching offer can successfully nudge people to donate, it doesn’t influence how generous they are once they’ve made that decision. The effect seems to operate on whether someone acts, not how much they give—an important insight for designing effective fundraising strategies."
  },
  {
    "objectID": "Blog/Project 1/index.html#simulation-experiment",
    "href": "Blog/Project 1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(42)\n\ncontrol &lt;- rbinom(10000, size = 1, prob = 0.018)\ntreatment &lt;- rbinom(10000, size = 1, prob = 0.022)\n\ndiffs &lt;- treatment - control\n\ncumulative_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\nlibrary(ggplot2)\nplot_data &lt;- data.frame(\n  Draw = 1:10000,\n  CumulativeAverage = cumulative_avg\n)\n\nggplot(plot_data, aes(x = Draw, y = CumulativeAverage)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_hline(yintercept = 0.004, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Cumulative Average of Treatment - Control Donations (Simulated)\",\n    subtitle = \"Demonstrating the Law of Large Numbers\",\n    x = \"Number of Simulations\",\n    y = \"Cumulative Average of Differences\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis simulation clearly demonstrates the Law of Large Numbers in action.\nAs we simulate more and more observations, the cumulative average difference in donation rates between the treatment and control groups steadily converges to the true expected value of 0.004. Early on, we see a lot of noise—the line bounces around as randomness dominates small sample sizes. But as the number of simulations increases, the fluctuations smooth out and the line settles near the true value.\nThis convergence gives us confidence in the reliability of our estimates. The more data we collect, the closer we get to the underlying truth—even when individual observations are binary and noisy, like whether someone donates.\n\n\nCentral Limit Theorem\n\nset.seed(123)\n\nsample_sizes &lt;- c(50, 200, 500, 1000)\nnum_simulations &lt;- 1000\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ tibble  3.1.5     ✔ purrr   1.0.2\n✔ tidyr   1.1.4     ✔ stringr 1.4.0\n✔ readr   2.0.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nsim_results &lt;- lapply(sample_sizes, function(n) {\n  diffs &lt;- replicate(num_simulations, {\n    treat &lt;- rbinom(n, size = 1, prob = 0.022)\n    control &lt;- rbinom(n, size = 1, prob = 0.018)\n    mean(treat) - mean(control)\n  })\n  tibble(\n    diff = diffs,\n    sample_size = n\n  )\n}) %&gt;% bind_rows()\n\n\nggplot(sim_results, aes(x = diff)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  facet_wrap(~ sample_size, scales = \"free\", ncol = 2) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Distribution of Simulated Mean Differences\",\n    subtitle = \"Across Varying Sample Sizes (Central Limit Theorem)\",\n    x = \"Average Treatment - Control\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe histograms below illustrate the Central Limit Theorem (CLT) in action.\nFor each sample size—50, 200, 500, and 1000—I repeatedly simulated the average difference in donation rates between a treatment group (with donation probability = 0.022) and a control group (0.018). Each histogram shows the distribution of these simulated mean differences.\nAs the sample size increases, the distribution becomes smoother, more symmetric, and narrower. With only 50 observations, the results are jagged and spread out—random chance can easily mask the treatment effect. But by the time we reach 1000 observations, the distribution clusters tightly around the true value (≈ 0.004), and resembles a classic bell curve.\nThe red dashed line at zero represents the “no effect” case. In small samples, it’s often close to the center of the distribution. But in large samples, zero moves to the tail, meaning the evidence increasingly rules out no effect.\nThis is exactly why the CLT is so powerful in applied work: with large enough samples, our estimates become well-behaved, normally distributed, and centered near the truth, even when the raw data is binary and highly skewed."
  },
  {
    "objectID": "Blog/Project 1/index.html#conclusion",
    "href": "Blog/Project 1/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis replication of Karlan and List (2007) confirmed several key insights about how simple behavioral cues can shape charitable giving.\nI was especially struck by how just the presence of a match—regardless of its size—was enough to increase donation rates. Whether the offer was 1:1 or 3:1, donors were equally likely to give. What mattered most was the framing: knowing their donation would be matched seemed to motivate action.\nInterestingly, once people decided to donate, the matching offer didn’t affect how much they gave. This suggests that the treatment primarily affects the decision to give (the extensive margin), rather than the donation amount (the intensive margin).\nAs someone studying data science and business analytics, this project reminded me that small psychological shifts can lead to measurable real-world effects. For fundraisers or policymakers, it’s a strong case for thoughtful design and messaging.\nSometimes, it’s not about offering more. It’s about offering it more effectively."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nCecelia Liu\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nCecelia Liu\n\n\nMay 4, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog/Project 2/hw2_questions.html",
    "href": "Blog/Project 2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nHere’s a preview of the data that we will be using:\n\nlibrary(readr)\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty,10)\n\n# A tibble: 10 × 4\n   patents region      age iscustomer\n     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1       0 Midwest    32.5          0\n 2       3 Southwest  37.5          0\n 3       4 Northwest  27            1\n 4       3 Northeast  24.5          0\n 5       3 Southwest  37            0\n 6       6 Northeast  29.5          1\n 7       5 Southwest  27            0\n 8       5 Northeast  20.5          0\n 9       6 Northeast  25            0\n10       4 Midwest    29.5          0\n\n\nAn now let’s do some data summaries:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nblueprinty %&gt;%\n  ggplot(aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 25) +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Number of Patents (last 5 years)\",\n    y     = \"Count\",\n    title = \"Patent Count Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents   = sd(patents),\n    n            = n()\n  )\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram and summary table together tell a clear story: firms using Blueprinty’s software (the orange bars and “1” in the table) tend to hold more patents than non-customers. You can see that the right-hand tail of the patent distribution is heavier for customers—meaning more high-patenting firms subscribe—and the mean patent count for customers (4.13) exceeds that of non-customers (3.47). In practical terms, Blueprinty may either attract or enable firms that are already more research-intensive. This baseline difference will be important to account for when we move on to our formal Poisson regression, since customer status appears correlated with prior patenting activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region proportions\nregion_prop &lt;- blueprinty %&gt;%\n  count(region, iscustomer) %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(region_prop, aes(x = region, y = prop, fill = factor(iscustomer))) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Region\",\n    y     = \"Proportion within Region\",\n    title = \"Regional Share of Customers vs. Non-customers\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Age distributions\nblueprinty %&gt;%\n  ggplot(aes(x = age, color = factor(iscustomer))) +\n  geom_density(size = 1) +\n  scale_color_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Firm Age (years)\",\n    y     = \"Density\",\n    title = \"Age Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe regional bar chart shows that Blueprinty’s customer base isn’t uniformly spread: the Northeast accounts for a noticeably larger share of subscribers (around 55%) compared with non-customers (about 45%), while the Midwest, Northwest, South, and Southwest all skew toward non-customers. This suggests Blueprinty’s marketing or network effects may be strongest in the Northeast tech corridor. Meanwhile, the age-density plot reveals that subscribing firms tend to cluster slightly younger—peaking around 20–25 years old—whereas non-customers exhibit a sharper concentration near 25–30 years. In other words, Blueprinty seems to appeal more to relatively early-stage engineering firms, especially those in the Northeast, which hints that any regression model should control for both firm age and region to avoid conflating customer effects with these underlying patterns.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, we model each firm’s patent count as\n\\[\nY_i \\sim \\mathrm{Poisson}(\\lambda),\n\\]\nwhere () is the (constant) mean patenting rate. The Poisson density for one observation is\n\\[\nf(y_i\\mid \\lambda)\n  = \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}.\n\\]\nAssuming independence across all (n) firms, the joint likelihood is\n\\[\nL(\\lambda; y_1, \\dots, y_n)\n= \\prod_{i=1}^n \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n= e^{-n\\lambda}\\,\\lambda^{\\sum_i y_i}\\,\\Bigl(\\prod_i y_i!\\Bigr)^{-1},\n\\]\nand the log‐likelihood becomes\n\\[\n\\ell(\\lambda)\n= \\sum_{i=1}^n \\bigl[y_i\\log\\lambda - \\lambda - \\log(y_i!)\\bigr]\n= -n\\lambda + \\Bigl(\\sum_i y_i\\Bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!).\n\\] The log-likelihood fuction for the Poisson Model is displayed below:\n\npoisson_loglikelihood &lt;- function(lambda, y) {\n  n_y  &lt;- length(y)\n  sum_y &lt;- sum(y)\n  ll &lt;- sum_y * log(lambda) - n_y * lambda - sum(lgamma(y + 1))\n  return(ll)\n}\n\n\n\nNow that we have our poisson_loglikelihood() function, let’s see how the log-likelihood behaves as we vary (). We’ll compute (()) for a grid of candidate rates and plot it.\n\n# prepare grid of lambda values\nlambda_grid &lt;- seq(0.5, 8, length.out = 200)\n\n# compute log-likelihoods\nll_values &lt;- sapply(lambda_grid, poisson_loglikelihood, \n                    y = blueprinty$patents)\n\n# put into a tibble for ggplot\nlibrary(tibble)\nll_df &lt;- tibble(lambda = lambda_grid, loglik = ll_values)\n\nlibrary(ggplot2)\nggplot(ll_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  labs(\n    x     = expression(lambda),\n    y     = \"Log-Likelihood\",\n    title = expression(paste(\"Log-Likelihood of Poisson(\", lambda, \")\"))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe curve peaks at the value of () that best fits our data—i.e. the maximum of (()). We’ll see that this peak occurs right around the sample mean of patent counts, which brings us to the next mathematical insight.\nIf you’re feeling mathematical, we can find the maximizer of the log‐likelihood by taking its derivative with respect to ():\n\\[\n\\frac{d}{d\\lambda}\\,\\ell(\\lambda)\n= \\frac{d}{d\\lambda}\\Bigl[-\\,n\\lambda + \\bigl(\\sum_{i=1}^n y_i\\bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!)\\Bigr]\n= -\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\\,.\n\\]\nSetting this equal to zero and solving gives:\n\\[\n-\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda_{\\rm MLE}\n= \\frac{1}{n}\\sum_{i=1}^n y_i\n= \\bar y.\n\\]\nThus, the maximum‐likelihood estimate of () is simply the sample mean of the observed patent counts.\n\n\n\nAnd here we can find the MLE by optimizing the likelihood fuction:\n\nres &lt;- optim(\n  par    = mean(blueprinty$patents), \n  fn     = function(l) -poisson_loglikelihood(l, blueprinty$patents),\n  method = \"Brent\",\n  lower  = 0.01, \n  upper  = 10\n)\n\nres$par\n\n[1] 3.684667\n\n\nAnd this is our numerical lambda_hat.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe use a log-link function to ensure ( _i &gt; 0 ), so that the model becomes linear on the log scale.\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta    &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\nWe now build the design matrix and use optim() to estimate ( ) and its standard errors using the Hessian matrix.\n\n# build design matrix (first column = intercept)\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\n# initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# maximize log-likelihood via optim() (minimize negative)\nreg_fit &lt;- optim(\n  par    = beta_init,\n  fn     = function(b) -poisson_regression_loglik(b, Y, X),\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract estimates and standard errors\nbeta_hat &lt;- reg_fit$par\nse_hat   &lt;- sqrt(diag(solve(reg_fit$hessian)))\n\nlibrary(knitr)\nkable(\n  data.frame(\n    Term     = colnames(X),\n    Estimate = beta_hat,\n    StdError = se_hat\n  ),\n  digits = 3\n)\n\n\n\n\nTerm\nEstimate\nStdError\n\n\n\n\n(Intercept)\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nI(age^2)\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\nTo validate our custom implementation, we compare the results with R’s built-in glm() function:\n\n# sanity check with built-in glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  family = poisson(link = \"log\"),\n  data   = blueprinty\n)\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0708  -0.9065  -0.1514   0.5808   4.8421  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe intercept captures the log-rate for a baseline firm (age zero, base region, non-customer).\nThe age and age-squared terms capture a nonlinear relationship between firm age and patent output.\nThe region coefficients show differences in patenting activity across regions (relative to the omitted category).\nThe iscustomer coefficient tells us the multiplicative effect of Blueprinty subscription on expected patent counts. Specifically, ( (_{}) ) gives the factor by which patenting increases for Blueprinty users.\n\nSince coefficients in a log-linear model aren’t directly interpretable in level terms, we simulate the treatment effect by comparing predicted patent counts with and without Blueprinty subscription for each firm.\n\n# if your data really lives in 'blueprint', do this:\nmodel    &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                family = poisson(link=\"log\"),\n                data   = blueprinty)\n\ndata_0   &lt;- blueprinty\ndata_1   &lt;- blueprinty\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect   &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average effect of being a customer on patents:\", round(effect,4), \"\\n\")\n\nAverage effect of being a customer on patents: 0.7928 \n\n\nAverage treatment‐effect of Blueprinty subscription\nThe average treatment effect — defined as:\n\\[\n\\mathbb{E}[\\hat\\lambda_i(\\text{iscustomer}=1) - \\hat\\lambda_i(\\text{iscustomer}=0)]\n\\]\n— is approximately 0.79 additional patents over five years, holding firm age and region constant."
  },
  {
    "objectID": "Blog/Project 2/hw2_questions.html#blueprinty-case-study",
    "href": "Blog/Project 2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nHere’s a preview of the data that we will be using:\n\nlibrary(readr)\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty,10)\n\n# A tibble: 10 × 4\n   patents region      age iscustomer\n     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1       0 Midwest    32.5          0\n 2       3 Southwest  37.5          0\n 3       4 Northwest  27            1\n 4       3 Northeast  24.5          0\n 5       3 Southwest  37            0\n 6       6 Northeast  29.5          1\n 7       5 Southwest  27            0\n 8       5 Northeast  20.5          0\n 9       6 Northeast  25            0\n10       4 Midwest    29.5          0\n\n\nAn now let’s do some data summaries:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nblueprinty %&gt;%\n  ggplot(aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 25) +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Number of Patents (last 5 years)\",\n    y     = \"Count\",\n    title = \"Patent Count Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents   = sd(patents),\n    n            = n()\n  )\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram and summary table together tell a clear story: firms using Blueprinty’s software (the orange bars and “1” in the table) tend to hold more patents than non-customers. You can see that the right-hand tail of the patent distribution is heavier for customers—meaning more high-patenting firms subscribe—and the mean patent count for customers (4.13) exceeds that of non-customers (3.47). In practical terms, Blueprinty may either attract or enable firms that are already more research-intensive. This baseline difference will be important to account for when we move on to our formal Poisson regression, since customer status appears correlated with prior patenting activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region proportions\nregion_prop &lt;- blueprinty %&gt;%\n  count(region, iscustomer) %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(region_prop, aes(x = region, y = prop, fill = factor(iscustomer))) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Region\",\n    y     = \"Proportion within Region\",\n    title = \"Regional Share of Customers vs. Non-customers\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Age distributions\nblueprinty %&gt;%\n  ggplot(aes(x = age, color = factor(iscustomer))) +\n  geom_density(size = 1) +\n  scale_color_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Firm Age (years)\",\n    y     = \"Density\",\n    title = \"Age Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe regional bar chart shows that Blueprinty’s customer base isn’t uniformly spread: the Northeast accounts for a noticeably larger share of subscribers (around 55%) compared with non-customers (about 45%), while the Midwest, Northwest, South, and Southwest all skew toward non-customers. This suggests Blueprinty’s marketing or network effects may be strongest in the Northeast tech corridor. Meanwhile, the age-density plot reveals that subscribing firms tend to cluster slightly younger—peaking around 20–25 years old—whereas non-customers exhibit a sharper concentration near 25–30 years. In other words, Blueprinty seems to appeal more to relatively early-stage engineering firms, especially those in the Northeast, which hints that any regression model should control for both firm age and region to avoid conflating customer effects with these underlying patterns.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, we model each firm’s patent count as\n\\[\nY_i \\sim \\mathrm{Poisson}(\\lambda),\n\\]\nwhere () is the (constant) mean patenting rate. The Poisson density for one observation is\n\\[\nf(y_i\\mid \\lambda)\n  = \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}.\n\\]\nAssuming independence across all (n) firms, the joint likelihood is\n\\[\nL(\\lambda; y_1, \\dots, y_n)\n= \\prod_{i=1}^n \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n= e^{-n\\lambda}\\,\\lambda^{\\sum_i y_i}\\,\\Bigl(\\prod_i y_i!\\Bigr)^{-1},\n\\]\nand the log‐likelihood becomes\n\\[\n\\ell(\\lambda)\n= \\sum_{i=1}^n \\bigl[y_i\\log\\lambda - \\lambda - \\log(y_i!)\\bigr]\n= -n\\lambda + \\Bigl(\\sum_i y_i\\Bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!).\n\\] The log-likelihood fuction for the Poisson Model is displayed below:\n\npoisson_loglikelihood &lt;- function(lambda, y) {\n  n_y  &lt;- length(y)\n  sum_y &lt;- sum(y)\n  ll &lt;- sum_y * log(lambda) - n_y * lambda - sum(lgamma(y + 1))\n  return(ll)\n}\n\n\n\nNow that we have our poisson_loglikelihood() function, let’s see how the log-likelihood behaves as we vary (). We’ll compute (()) for a grid of candidate rates and plot it.\n\n# prepare grid of lambda values\nlambda_grid &lt;- seq(0.5, 8, length.out = 200)\n\n# compute log-likelihoods\nll_values &lt;- sapply(lambda_grid, poisson_loglikelihood, \n                    y = blueprinty$patents)\n\n# put into a tibble for ggplot\nlibrary(tibble)\nll_df &lt;- tibble(lambda = lambda_grid, loglik = ll_values)\n\nlibrary(ggplot2)\nggplot(ll_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  labs(\n    x     = expression(lambda),\n    y     = \"Log-Likelihood\",\n    title = expression(paste(\"Log-Likelihood of Poisson(\", lambda, \")\"))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe curve peaks at the value of () that best fits our data—i.e. the maximum of (()). We’ll see that this peak occurs right around the sample mean of patent counts, which brings us to the next mathematical insight.\nIf you’re feeling mathematical, we can find the maximizer of the log‐likelihood by taking its derivative with respect to ():\n\\[\n\\frac{d}{d\\lambda}\\,\\ell(\\lambda)\n= \\frac{d}{d\\lambda}\\Bigl[-\\,n\\lambda + \\bigl(\\sum_{i=1}^n y_i\\bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!)\\Bigr]\n= -\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\\,.\n\\]\nSetting this equal to zero and solving gives:\n\\[\n-\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda_{\\rm MLE}\n= \\frac{1}{n}\\sum_{i=1}^n y_i\n= \\bar y.\n\\]\nThus, the maximum‐likelihood estimate of () is simply the sample mean of the observed patent counts.\n\n\n\nAnd here we can find the MLE by optimizing the likelihood fuction:\n\nres &lt;- optim(\n  par    = mean(blueprinty$patents), \n  fn     = function(l) -poisson_loglikelihood(l, blueprinty$patents),\n  method = \"Brent\",\n  lower  = 0.01, \n  upper  = 10\n)\n\nres$par\n\n[1] 3.684667\n\n\nAnd this is our numerical lambda_hat.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe use a log-link function to ensure ( _i &gt; 0 ), so that the model becomes linear on the log scale.\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta    &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\nWe now build the design matrix and use optim() to estimate ( ) and its standard errors using the Hessian matrix.\n\n# build design matrix (first column = intercept)\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\n# initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# maximize log-likelihood via optim() (minimize negative)\nreg_fit &lt;- optim(\n  par    = beta_init,\n  fn     = function(b) -poisson_regression_loglik(b, Y, X),\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract estimates and standard errors\nbeta_hat &lt;- reg_fit$par\nse_hat   &lt;- sqrt(diag(solve(reg_fit$hessian)))\n\nlibrary(knitr)\nkable(\n  data.frame(\n    Term     = colnames(X),\n    Estimate = beta_hat,\n    StdError = se_hat\n  ),\n  digits = 3\n)\n\n\n\n\nTerm\nEstimate\nStdError\n\n\n\n\n(Intercept)\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nI(age^2)\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\nTo validate our custom implementation, we compare the results with R’s built-in glm() function:\n\n# sanity check with built-in glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  family = poisson(link = \"log\"),\n  data   = blueprinty\n)\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0708  -0.9065  -0.1514   0.5808   4.8421  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe intercept captures the log-rate for a baseline firm (age zero, base region, non-customer).\nThe age and age-squared terms capture a nonlinear relationship between firm age and patent output.\nThe region coefficients show differences in patenting activity across regions (relative to the omitted category).\nThe iscustomer coefficient tells us the multiplicative effect of Blueprinty subscription on expected patent counts. Specifically, ( (_{}) ) gives the factor by which patenting increases for Blueprinty users.\n\nSince coefficients in a log-linear model aren’t directly interpretable in level terms, we simulate the treatment effect by comparing predicted patent counts with and without Blueprinty subscription for each firm.\n\n# if your data really lives in 'blueprint', do this:\nmodel    &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                family = poisson(link=\"log\"),\n                data   = blueprinty)\n\ndata_0   &lt;- blueprinty\ndata_1   &lt;- blueprinty\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect   &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average effect of being a customer on patents:\", round(effect,4), \"\\n\")\n\nAverage effect of being a customer on patents: 0.7928 \n\n\nAverage treatment‐effect of Blueprinty subscription\nThe average treatment effect — defined as:\n\\[\n\\mathbb{E}[\\hat\\lambda_i(\\text{iscustomer}=1) - \\hat\\lambda_i(\\text{iscustomer}=0)]\n\\]\n— is approximately 0.79 additional patents over five years, holding firm age and region constant."
  },
  {
    "objectID": "Blog/Project 2/hw2_questions.html#airbnb-case-study",
    "href": "Blog/Project 2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe begin by loading the dataset and inspecting the first few rows:\n\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nhead(airbnb,10)\n\n# A tibble: 10 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     3  3647  3050 4/2/2017     11/25/2008 Private r…         1        1   150\n 4     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 5     5  4611  3012 4/2/2017     1/2/2009   Private r…        NA        1    39\n 6     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 7     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 8     8  5121  2980 4/2/2017     2/3/2009   Private r…        NA        1    60\n 9     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n10    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n# … with 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;lgl&gt;\n\n\nFrom the preview, we can see that the data includes numerical variables (e.g., price, days, bedrooms, number_of_reviews) and categorical variables (room_type, instant_bookable). Some values such as bathrooms contain missing entries.\n\n\n\nData Cleaning\nWe drop observations with missing values in key predictor or outcome variables to ensure model reliability.\n\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(!is.na(bathrooms),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(number_of_reviews),\n         !is.na(bedrooms),\n         !is.na(price),\n         !is.na(room_type),\n         !is.na(instant_bookable))\n\nairbnb_clean\n\n# A tibble: 30,160 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 4     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 5     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 6     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n 7    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n 8    11  5203  2978 4/2/2017     2/5/2009   Private r…         1        1    79\n 9    12  5238  2976 4/2/2017     2/7/2009   Entire ho…         1        1   140\n10    13  5441  2967 4/2/2017     2/16/2009  Private r…         1        1   126\n# … with 30,150 more rows, and 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;lgl&gt;\n\n\n\n\n\nPoisson Regression: Modeling Review Counts\nWe assume the number of reviews follows a Poisson distribution and model it using firm-level characteristics. Specifically, we model:\n\\[\n\\mathbb{E}[Y_i \\mid X_i] = \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nwhere (Y_i) is the number of reviews for listing (i), and (X_i) includes: - days listed on platform - price - bedrooms - bathrooms - review_scores_cleanliness, location, value - room_type (categorical) - instant_bookable (binary)\nWe use the glm() function with a log link and Poisson family:\n\nairbnb_clean$instant_bookable &lt;- ifelse(airbnb_clean$instant_bookable == \"TRUE\", 1, 0)\nhead(airbnb_clean,20)\n\n# A tibble: 20 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 4     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 5     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 6     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n 7    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n 8    11  5203  2978 4/2/2017     2/5/2009   Private r…         1        1    79\n 9    12  5238  2976 4/2/2017     2/7/2009   Entire ho…         1        1   140\n10    13  5441  2967 4/2/2017     2/16/2009  Private r…         1        1   126\n11    15  5857  2941 4/2/2017     3/14/2009  Entire ho…         2        2   215\n12    16  5945  3063 4/2/2017     11/12/2008 Private r…         1        1   120\n13    18  6990  2882 4/2/2017     5/12/2009  Private r…         1        1    95\n14    19  7036  2699 4/2/2017     11/11/2009 Private r…         1        1    59\n15    20  7097  2877 4/2/2017     5/17/2009  Entire ho…         1        1   215\n16    21  7322  2867 4/2/2017     5/27/2009  Private r…         1        1   120\n17    23  7816  2851 4/2/2017     6/12/2009  Entire ho…         1        1   225\n18    24  7852  2981 4/3/2017     2/3/2009   Entire ho…         1        1   250\n19    25  8024  2843 4/2/2017     6/20/2009  Private r…         3        1   126\n20    26  8025  2843 4/2/2017     6/20/2009  Private r…         1        1   110\n# … with 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;dbl&gt;\n\n\n\nmodel &lt;- glm(\n  number_of_reviews ~ days + price + bedrooms + bathrooms +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    room_type + instant_bookable,\n  family = poisson(link = \"log\"),\n  data = airbnb_clean\n)\n\nsummary(model)\n\n\nCall:\nglm(formula = number_of_reviews ~ days + price + bedrooms + bathrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    room_type + instant_bookable, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-21.096   -4.804   -3.001    0.961   38.630  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.498e+00  1.609e-02 217.396  &lt; 2e-16 ***\ndays                       5.072e-05  3.909e-07 129.757  &lt; 2e-16 ***\nprice                     -1.791e-05  8.327e-06  -2.151 0.031485 *  \nbedrooms                   7.409e-02  1.992e-03  37.197  &lt; 2e-16 ***\nbathrooms                 -1.177e-01  3.749e-03 -31.394  &lt; 2e-16 ***\nreview_scores_cleanliness  1.131e-01  1.496e-03  75.611  &lt; 2e-16 ***\nreview_scores_location    -7.690e-02  1.609e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.108e-02  1.804e-03 -50.490  &lt; 2e-16 ***\nroom_typePrivate room     -1.054e-02  2.738e-03  -3.847 0.000119 ***\nroom_typeShared room      -2.463e-01  8.620e-03 -28.578  &lt; 2e-16 ***\ninstant_bookable           3.459e-01  2.890e-03 119.666  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 926886  on 30149  degrees of freedom\nAIC: 1048375\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nInterpreting the Coefficients\n\nPositive coefficients imply an increase in the expected number of reviews (e.g., longer time on platform, higher cleanliness or location ratings).\nNegative coefficients imply a decrease (e.g., higher price might deter bookings).\nCategorical variables like room_type are interpreted relative to a base level (likely “Entire home/apt”).\nThe coefficient on instant_bookable tells us whether being instantly bookable increases expected bookings.\n\nAll effects are multiplicative on the count scale. For example, if the coefficient for cleanliness is 0.08, then a one-point increase in cleanliness score is associated with a ( e^{0.08} ) or 8% increase in expected review count.\n\n\n\nSummary\nIn this case study, we analyzed a dataset of 40,000 Airbnb listings in New York City to understand what factors drive the number of reviews a listing receives. After cleaning the data to remove missing values, we used a Poisson regression model to estimate the expected number of reviews as a function of listing attributes such as duration on the platform, price, room type, and review scores.\nThe results suggest that: - Listings that have been active longer, have more bedrooms, and receive higher cleanliness ratings tend to attract more reviews. - Higher prices and shared room types are linked to fewer reviews. - Most effects are statistically significant and directionally plausible.\nOverall, the model highlights actionable levers that hosts might optimize to improve visibility and booking performance on the Airbnb platform. ains variation in review counts with a large sample size (n ≈ 30,000), and all variables except instant_bookable are statistically significant at the 0.001 level."
  },
  {
    "objectID": "Blog/Project 2/index.html",
    "href": "Blog/Project 2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nHere’s a preview of the data that we will be using:\n\nlibrary(readr)\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty,10)\n\n# A tibble: 10 × 4\n   patents region      age iscustomer\n     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1       0 Midwest    32.5          0\n 2       3 Southwest  37.5          0\n 3       4 Northwest  27            1\n 4       3 Northeast  24.5          0\n 5       3 Southwest  37            0\n 6       6 Northeast  29.5          1\n 7       5 Southwest  27            0\n 8       5 Northeast  20.5          0\n 9       6 Northeast  25            0\n10       4 Midwest    29.5          0\n\n\nAn now let’s do some data summaries:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nblueprinty %&gt;%\n  ggplot(aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 25) +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Number of Patents (last 5 years)\",\n    y     = \"Count\",\n    title = \"Patent Count Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents   = sd(patents),\n    n            = n()\n  )\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram and summary table together tell a clear story: firms using Blueprinty’s software (the orange bars and “1” in the table) tend to hold more patents than non-customers. You can see that the right-hand tail of the patent distribution is heavier for customers—meaning more high-patenting firms subscribe—and the mean patent count for customers (4.13) exceeds that of non-customers (3.47). In practical terms, Blueprinty may either attract or enable firms that are already more research-intensive. This baseline difference will be important to account for when we move on to our formal Poisson regression, since customer status appears correlated with prior patenting activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region proportions\nregion_prop &lt;- blueprinty %&gt;%\n  count(region, iscustomer) %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(region_prop, aes(x = region, y = prop, fill = factor(iscustomer))) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Region\",\n    y     = \"Proportion within Region\",\n    title = \"Regional Share of Customers vs. Non-customers\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Age distributions\nblueprinty %&gt;%\n  ggplot(aes(x = age, color = factor(iscustomer))) +\n  geom_density(size = 1) +\n  scale_color_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Firm Age (years)\",\n    y     = \"Density\",\n    title = \"Age Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe regional bar chart shows that Blueprinty’s customer base isn’t uniformly spread: the Northeast accounts for a noticeably larger share of subscribers (around 55%) compared with non-customers (about 45%), while the Midwest, Northwest, South, and Southwest all skew toward non-customers. This suggests Blueprinty’s marketing or network effects may be strongest in the Northeast tech corridor. Meanwhile, the age-density plot reveals that subscribing firms tend to cluster slightly younger—peaking around 20–25 years old—whereas non-customers exhibit a sharper concentration near 25–30 years. In other words, Blueprinty seems to appeal more to relatively early-stage engineering firms, especially those in the Northeast, which hints that any regression model should control for both firm age and region to avoid conflating customer effects with these underlying patterns.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, we model each firm’s patent count as\n\\[\nY_i \\sim \\mathrm{Poisson}(\\lambda),\n\\]\nwhere () is the (constant) mean patenting rate. The Poisson density for one observation is\n\\[\nf(y_i\\mid \\lambda)\n  = \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}.\n\\]\nAssuming independence across all (n) firms, the joint likelihood is\n\\[\nL(\\lambda; y_1, \\dots, y_n)\n= \\prod_{i=1}^n \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n= e^{-n\\lambda}\\,\\lambda^{\\sum_i y_i}\\,\\Bigl(\\prod_i y_i!\\Bigr)^{-1},\n\\]\nand the log‐likelihood becomes\n\\[\n\\ell(\\lambda)\n= \\sum_{i=1}^n \\bigl[y_i\\log\\lambda - \\lambda - \\log(y_i!)\\bigr]\n= -n\\lambda + \\Bigl(\\sum_i y_i\\Bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!).\n\\] The log-likelihood fuction for the Poisson Model is displayed below:\n\npoisson_loglikelihood &lt;- function(lambda, y) {\n  n_y  &lt;- length(y)\n  sum_y &lt;- sum(y)\n  ll &lt;- sum_y * log(lambda) - n_y * lambda - sum(lgamma(y + 1))\n  return(ll)\n}\n\n\n\nNow that we have our poisson_loglikelihood() function, let’s see how the log-likelihood behaves as we vary (). We’ll compute (()) for a grid of candidate rates and plot it.\n\n# prepare grid of lambda values\nlambda_grid &lt;- seq(0.5, 8, length.out = 200)\n\n# compute log-likelihoods\nll_values &lt;- sapply(lambda_grid, poisson_loglikelihood, \n                    y = blueprinty$patents)\n\n# put into a tibble for ggplot\nlibrary(tibble)\nll_df &lt;- tibble(lambda = lambda_grid, loglik = ll_values)\n\nlibrary(ggplot2)\nggplot(ll_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  labs(\n    x     = expression(lambda),\n    y     = \"Log-Likelihood\",\n    title = expression(paste(\"Log-Likelihood of Poisson(\", lambda, \")\"))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe curve peaks at the value of () that best fits our data—i.e. the maximum of (()). We’ll see that this peak occurs right around the sample mean of patent counts, which brings us to the next mathematical insight.\nIf you’re feeling mathematical, we can find the maximizer of the log‐likelihood by taking its derivative with respect to ():\n\\[\n\\frac{d}{d\\lambda}\\,\\ell(\\lambda)\n= \\frac{d}{d\\lambda}\\Bigl[-\\,n\\lambda + \\bigl(\\sum_{i=1}^n y_i\\bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!)\\Bigr]\n= -\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\\,.\n\\]\nSetting this equal to zero and solving gives:\n\\[\n-\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda_{\\rm MLE}\n= \\frac{1}{n}\\sum_{i=1}^n y_i\n= \\bar y.\n\\]\nThus, the maximum‐likelihood estimate of () is simply the sample mean of the observed patent counts.\n\n\n\nAnd here we can find the MLE by optimizing the likelihood fuction:\n\nres &lt;- optim(\n  par    = mean(blueprinty$patents), \n  fn     = function(l) -poisson_loglikelihood(l, blueprinty$patents),\n  method = \"Brent\",\n  lower  = 0.01, \n  upper  = 10\n)\n\nres$par\n\n[1] 3.684667\n\n\nAnd this is our numerical lambda_hat.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe use a log-link function to ensure ( _i &gt; 0 ), so that the model becomes linear on the log scale.\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta    &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\nWe now build the design matrix and use optim() to estimate ( ) and its standard errors using the Hessian matrix.\n\n# build design matrix (first column = intercept)\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\n# initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# maximize log-likelihood via optim() (minimize negative)\nreg_fit &lt;- optim(\n  par    = beta_init,\n  fn     = function(b) -poisson_regression_loglik(b, Y, X),\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract estimates and standard errors\nbeta_hat &lt;- reg_fit$par\nse_hat   &lt;- sqrt(diag(solve(reg_fit$hessian)))\n\nlibrary(knitr)\nkable(\n  data.frame(\n    Term     = colnames(X),\n    Estimate = beta_hat,\n    StdError = se_hat\n  ),\n  digits = 3\n)\n\n\n\n\nTerm\nEstimate\nStdError\n\n\n\n\n(Intercept)\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nI(age^2)\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\nTo validate our custom implementation, we compare the results with R’s built-in glm() function:\n\n# sanity check with built-in glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  family = poisson(link = \"log\"),\n  data   = blueprinty\n)\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0708  -0.9065  -0.1514   0.5808   4.8421  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe intercept captures the log-rate for a baseline firm (age zero, base region, non-customer).\nThe age and age-squared terms capture a nonlinear relationship between firm age and patent output.\nThe region coefficients show differences in patenting activity across regions (relative to the omitted category).\nThe iscustomer coefficient tells us the multiplicative effect of Blueprinty subscription on expected patent counts. Specifically, ( (_{}) ) gives the factor by which patenting increases for Blueprinty users.\n\nSince coefficients in a log-linear model aren’t directly interpretable in level terms, we simulate the treatment effect by comparing predicted patent counts with and without Blueprinty subscription for each firm.\n\n# if your data really lives in 'blueprint', do this:\nmodel    &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                family = poisson(link=\"log\"),\n                data   = blueprinty)\n\ndata_0   &lt;- blueprinty\ndata_1   &lt;- blueprinty\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect   &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average effect of being a customer on patents:\", round(effect,4), \"\\n\")\n\nAverage effect of being a customer on patents: 0.7928 \n\n\nAverage treatment‐effect of Blueprinty subscription\nThe average treatment effect — defined as:\n\\[\n\\mathbb{E}[\\hat\\lambda_i(\\text{iscustomer}=1) - \\hat\\lambda_i(\\text{iscustomer}=0)]\n\\]\n— is approximately 0.79 additional patents over five years, holding firm age and region constant."
  },
  {
    "objectID": "Blog/Project 2/index.html#blueprinty-case-study",
    "href": "Blog/Project 2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nHere’s a preview of the data that we will be using:\n\nlibrary(readr)\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty,10)\n\n# A tibble: 10 × 4\n   patents region      age iscustomer\n     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1       0 Midwest    32.5          0\n 2       3 Southwest  37.5          0\n 3       4 Northwest  27            1\n 4       3 Northeast  24.5          0\n 5       3 Southwest  37            0\n 6       6 Northeast  29.5          1\n 7       5 Southwest  27            0\n 8       5 Northeast  20.5          0\n 9       6 Northeast  25            0\n10       4 Midwest    29.5          0\n\n\nAn now let’s do some data summaries:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nblueprinty %&gt;%\n  ggplot(aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.6, bins = 25) +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Number of Patents (last 5 years)\",\n    y     = \"Count\",\n    title = \"Patent Count Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents   = sd(patents),\n    n            = n()\n  )\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n\nThe histogram and summary table together tell a clear story: firms using Blueprinty’s software (the orange bars and “1” in the table) tend to hold more patents than non-customers. You can see that the right-hand tail of the patent distribution is heavier for customers—meaning more high-patenting firms subscribe—and the mean patent count for customers (4.13) exceeds that of non-customers (3.47). In practical terms, Blueprinty may either attract or enable firms that are already more research-intensive. This baseline difference will be important to account for when we move on to our formal Poisson regression, since customer status appears correlated with prior patenting activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region proportions\nregion_prop &lt;- blueprinty %&gt;%\n  count(region, iscustomer) %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(region_prop, aes(x = region, y = prop, fill = factor(iscustomer))) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Region\",\n    y     = \"Proportion within Region\",\n    title = \"Regional Share of Customers vs. Non-customers\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Age distributions\nblueprinty %&gt;%\n  ggplot(aes(x = age, color = factor(iscustomer))) +\n  geom_density(size = 1) +\n  scale_color_manual(\n    name   = \"Customer\",\n    values = c(\"#1f77b4\", \"#ff7f0e\"),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    x     = \"Firm Age (years)\",\n    y     = \"Density\",\n    title = \"Age Distribution by Customer Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe regional bar chart shows that Blueprinty’s customer base isn’t uniformly spread: the Northeast accounts for a noticeably larger share of subscribers (around 55%) compared with non-customers (about 45%), while the Midwest, Northwest, South, and Southwest all skew toward non-customers. This suggests Blueprinty’s marketing or network effects may be strongest in the Northeast tech corridor. Meanwhile, the age-density plot reveals that subscribing firms tend to cluster slightly younger—peaking around 20–25 years old—whereas non-customers exhibit a sharper concentration near 25–30 years. In other words, Blueprinty seems to appeal more to relatively early-stage engineering firms, especially those in the Northeast, which hints that any regression model should control for both firm age and region to avoid conflating customer effects with these underlying patterns.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, we model each firm’s patent count as\n\\[\nY_i \\sim \\mathrm{Poisson}(\\lambda),\n\\]\nwhere () is the (constant) mean patenting rate. The Poisson density for one observation is\n\\[\nf(y_i\\mid \\lambda)\n  = \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}.\n\\]\nAssuming independence across all (n) firms, the joint likelihood is\n\\[\nL(\\lambda; y_1, \\dots, y_n)\n= \\prod_{i=1}^n \\frac{e^{-\\lambda}\\,\\lambda^{y_i}}{y_i!}\n= e^{-n\\lambda}\\,\\lambda^{\\sum_i y_i}\\,\\Bigl(\\prod_i y_i!\\Bigr)^{-1},\n\\]\nand the log‐likelihood becomes\n\\[\n\\ell(\\lambda)\n= \\sum_{i=1}^n \\bigl[y_i\\log\\lambda - \\lambda - \\log(y_i!)\\bigr]\n= -n\\lambda + \\Bigl(\\sum_i y_i\\Bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!).\n\\] The log-likelihood fuction for the Poisson Model is displayed below:\n\npoisson_loglikelihood &lt;- function(lambda, y) {\n  n_y  &lt;- length(y)\n  sum_y &lt;- sum(y)\n  ll &lt;- sum_y * log(lambda) - n_y * lambda - sum(lgamma(y + 1))\n  return(ll)\n}\n\n\n\nNow that we have our poisson_loglikelihood() function, let’s see how the log-likelihood behaves as we vary (). We’ll compute (()) for a grid of candidate rates and plot it.\n\n# prepare grid of lambda values\nlambda_grid &lt;- seq(0.5, 8, length.out = 200)\n\n# compute log-likelihoods\nll_values &lt;- sapply(lambda_grid, poisson_loglikelihood, \n                    y = blueprinty$patents)\n\n# put into a tibble for ggplot\nlibrary(tibble)\nll_df &lt;- tibble(lambda = lambda_grid, loglik = ll_values)\n\nlibrary(ggplot2)\nggplot(ll_df, aes(x = lambda, y = loglik)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  labs(\n    x     = expression(lambda),\n    y     = \"Log-Likelihood\",\n    title = expression(paste(\"Log-Likelihood of Poisson(\", lambda, \")\"))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe curve peaks at the value of () that best fits our data—i.e. the maximum of (()). We’ll see that this peak occurs right around the sample mean of patent counts, which brings us to the next mathematical insight.\nIf you’re feeling mathematical, we can find the maximizer of the log‐likelihood by taking its derivative with respect to ():\n\\[\n\\frac{d}{d\\lambda}\\,\\ell(\\lambda)\n= \\frac{d}{d\\lambda}\\Bigl[-\\,n\\lambda + \\bigl(\\sum_{i=1}^n y_i\\bigr)\\log\\lambda - \\sum_{i=1}^n \\log(y_i!)\\Bigr]\n= -\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda}\\,.\n\\]\nSetting this equal to zero and solving gives:\n\\[\n-\\,n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad\\Longrightarrow\\quad\n\\hat\\lambda_{\\rm MLE}\n= \\frac{1}{n}\\sum_{i=1}^n y_i\n= \\bar y.\n\\]\nThus, the maximum‐likelihood estimate of () is simply the sample mean of the observed patent counts.\n\n\n\nAnd here we can find the MLE by optimizing the likelihood fuction:\n\nres &lt;- optim(\n  par    = mean(blueprinty$patents), \n  fn     = function(l) -poisson_loglikelihood(l, blueprinty$patents),\n  method = \"Brent\",\n  lower  = 0.01, \n  upper  = 10\n)\n\nres$par\n\n[1] 3.684667\n\n\nAnd this is our numerical lambda_hat.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe use a log-link function to ensure ( _i &gt; 0 ), so that the model becomes linear on the log scale.\n\npoisson_regression_loglik &lt;- function(beta, Y, X) {\n  eta    &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  sum(dpois(Y, lambda, log = TRUE))\n}\n\nWe now build the design matrix and use optim() to estimate ( ) and its standard errors using the Hessian matrix.\n\n# build design matrix (first column = intercept)\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\n# initial guess\nbeta_init &lt;- rep(0, ncol(X))\n\n# maximize log-likelihood via optim() (minimize negative)\nreg_fit &lt;- optim(\n  par    = beta_init,\n  fn     = function(b) -poisson_regression_loglik(b, Y, X),\n  hessian = TRUE,\n  method = \"BFGS\"\n)\n\n# extract estimates and standard errors\nbeta_hat &lt;- reg_fit$par\nse_hat   &lt;- sqrt(diag(solve(reg_fit$hessian)))\n\nlibrary(knitr)\nkable(\n  data.frame(\n    Term     = colnames(X),\n    Estimate = beta_hat,\n    StdError = se_hat\n  ),\n  digits = 3\n)\n\n\n\n\nTerm\nEstimate\nStdError\n\n\n\n\n(Intercept)\n-0.126\n0.112\n\n\nage\n0.116\n0.006\n\n\nI(age^2)\n-0.002\n0.000\n\n\nregionNortheast\n-0.025\n0.043\n\n\nregionNorthwest\n-0.035\n0.053\n\n\nregionSouth\n-0.005\n0.052\n\n\nregionSouthwest\n-0.038\n0.047\n\n\niscustomer\n0.061\n0.032\n\n\n\n\n\nTo validate our custom implementation, we compare the results with R’s built-in glm() function:\n\n# sanity check with built-in glm()\nglm_fit &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  family = poisson(link = \"log\"),\n  data   = blueprinty\n)\nsummary(glm_fit)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = blueprinty)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.0708  -0.9065  -0.1514   0.5808   4.8421  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nThe intercept captures the log-rate for a baseline firm (age zero, base region, non-customer).\nThe age and age-squared terms capture a nonlinear relationship between firm age and patent output.\nThe region coefficients show differences in patenting activity across regions (relative to the omitted category).\nThe iscustomer coefficient tells us the multiplicative effect of Blueprinty subscription on expected patent counts. Specifically, ( (_{}) ) gives the factor by which patenting increases for Blueprinty users.\n\nSince coefficients in a log-linear model aren’t directly interpretable in level terms, we simulate the treatment effect by comparing predicted patent counts with and without Blueprinty subscription for each firm.\n\n# if your data really lives in 'blueprint', do this:\nmodel    &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                family = poisson(link=\"log\"),\n                data   = blueprinty)\n\ndata_0   &lt;- blueprinty\ndata_1   &lt;- blueprinty\n\ndata_0$iscustomer &lt;- 0\ndata_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(model, newdata = data_0, type = \"response\")\ny_pred_1 &lt;- predict(model, newdata = data_1, type = \"response\")\n\neffect   &lt;- mean(y_pred_1 - y_pred_0)\ncat(\"Average effect of being a customer on patents:\", round(effect,4), \"\\n\")\n\nAverage effect of being a customer on patents: 0.7928 \n\n\nAverage treatment‐effect of Blueprinty subscription\nThe average treatment effect — defined as:\n\\[\n\\mathbb{E}[\\hat\\lambda_i(\\text{iscustomer}=1) - \\hat\\lambda_i(\\text{iscustomer}=0)]\n\\]\n— is approximately 0.79 additional patents over five years, holding firm age and region constant."
  },
  {
    "objectID": "Blog/Project 2/index.html#airbnb-case-study",
    "href": "Blog/Project 2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe begin by loading the dataset and inspecting the first few rows:\n\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nhead(airbnb,10)\n\n# A tibble: 10 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     3  3647  3050 4/2/2017     11/25/2008 Private r…         1        1   150\n 4     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 5     5  4611  3012 4/2/2017     1/2/2009   Private r…        NA        1    39\n 6     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 7     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 8     8  5121  2980 4/2/2017     2/3/2009   Private r…        NA        1    60\n 9     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n10    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n# … with 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;lgl&gt;\n\n\nFrom the preview, we can see that the data includes numerical variables (e.g., price, days, bedrooms, number_of_reviews) and categorical variables (room_type, instant_bookable). Some values such as bathrooms contain missing entries.\n\n\n\nData Cleaning\nWe drop observations with missing values in key predictor or outcome variables to ensure model reliability.\n\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(!is.na(bathrooms),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(number_of_reviews),\n         !is.na(bedrooms),\n         !is.na(price),\n         !is.na(room_type),\n         !is.na(instant_bookable))\n\nairbnb_clean\n\n# A tibble: 30,160 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 4     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 5     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 6     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n 7    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n 8    11  5203  2978 4/2/2017     2/5/2009   Private r…         1        1    79\n 9    12  5238  2976 4/2/2017     2/7/2009   Entire ho…         1        1   140\n10    13  5441  2967 4/2/2017     2/16/2009  Private r…         1        1   126\n# … with 30,150 more rows, and 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;lgl&gt;\n\n\n\n\n\nPoisson Regression: Modeling Review Counts\nWe assume the number of reviews follows a Poisson distribution and model it using firm-level characteristics. Specifically, we model:\n\\[\n\\mathbb{E}[Y_i \\mid X_i] = \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nwhere (Y_i) is the number of reviews for listing (i), and (X_i) includes: - days listed on platform - price - bedrooms - bathrooms - review_scores_cleanliness, location, value - room_type (categorical) - instant_bookable (binary)\nWe use the glm() function with a log link and Poisson family:\n\nairbnb_clean$instant_bookable &lt;- ifelse(airbnb_clean$instant_bookable == \"TRUE\", 1, 0)\nhead(airbnb_clean,20)\n\n# A tibble: 20 × 14\n    ...1    id  days last_scraped host_since room_type  bathrooms bedrooms price\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1  2515  3130 4/2/2017     9/6/2008   Private r…         1        1    59\n 2     2  2595  3127 4/2/2017     9/9/2008   Entire ho…         1        0   230\n 3     4  3831  3038 4/2/2017     12/7/2008  Entire ho…         1        1    89\n 4     6  5099  2981 4/2/2017     2/2/2009   Entire ho…         1        1   212\n 5     7  5107  2981 4/2/2017     2/2/2009   Entire ho…         1        2   250\n 6     9  5172  2980 4/2/2017     2/3/2009   Entire ho…         1        1   129\n 7    10  5178  2952 4/2/2017     3/3/2009   Private r…         1        1    79\n 8    11  5203  2978 4/2/2017     2/5/2009   Private r…         1        1    79\n 9    12  5238  2976 4/2/2017     2/7/2009   Entire ho…         1        1   140\n10    13  5441  2967 4/2/2017     2/16/2009  Private r…         1        1   126\n11    15  5857  2941 4/2/2017     3/14/2009  Entire ho…         2        2   215\n12    16  5945  3063 4/2/2017     11/12/2008 Private r…         1        1   120\n13    18  6990  2882 4/2/2017     5/12/2009  Private r…         1        1    95\n14    19  7036  2699 4/2/2017     11/11/2009 Private r…         1        1    59\n15    20  7097  2877 4/2/2017     5/17/2009  Entire ho…         1        1   215\n16    21  7322  2867 4/2/2017     5/27/2009  Private r…         1        1   120\n17    23  7816  2851 4/2/2017     6/12/2009  Entire ho…         1        1   225\n18    24  7852  2981 4/3/2017     2/3/2009   Entire ho…         1        1   250\n19    25  8024  2843 4/2/2017     6/20/2009  Private r…         3        1   126\n20    26  8025  2843 4/2/2017     6/20/2009  Private r…         1        1   110\n# … with 5 more variables: number_of_reviews &lt;dbl&gt;,\n#   review_scores_cleanliness &lt;dbl&gt;, review_scores_location &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, instant_bookable &lt;dbl&gt;\n\n\n\nmodel &lt;- glm(\n  number_of_reviews ~ days + price + bedrooms + bathrooms +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    room_type + instant_bookable,\n  family = poisson(link = \"log\"),\n  data = airbnb_clean\n)\n\nsummary(model)\n\n\nCall:\nglm(formula = number_of_reviews ~ days + price + bedrooms + bathrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    room_type + instant_bookable, family = poisson(link = \"log\"), \n    data = airbnb_clean)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-21.096   -4.804   -3.001    0.961   38.630  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.498e+00  1.609e-02 217.396  &lt; 2e-16 ***\ndays                       5.072e-05  3.909e-07 129.757  &lt; 2e-16 ***\nprice                     -1.791e-05  8.327e-06  -2.151 0.031485 *  \nbedrooms                   7.409e-02  1.992e-03  37.197  &lt; 2e-16 ***\nbathrooms                 -1.177e-01  3.749e-03 -31.394  &lt; 2e-16 ***\nreview_scores_cleanliness  1.131e-01  1.496e-03  75.611  &lt; 2e-16 ***\nreview_scores_location    -7.690e-02  1.609e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.108e-02  1.804e-03 -50.490  &lt; 2e-16 ***\nroom_typePrivate room     -1.054e-02  2.738e-03  -3.847 0.000119 ***\nroom_typeShared room      -2.463e-01  8.620e-03 -28.578  &lt; 2e-16 ***\ninstant_bookable           3.459e-01  2.890e-03 119.666  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 926886  on 30149  degrees of freedom\nAIC: 1048375\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nInterpreting the Coefficients\n\nPositive coefficients imply an increase in the expected number of reviews (e.g., longer time on platform, higher cleanliness or location ratings).\nNegative coefficients imply a decrease (e.g., higher price might deter bookings).\nCategorical variables like room_type are interpreted relative to a base level (likely “Entire home/apt”).\nThe coefficient on instant_bookable tells us whether being instantly bookable increases expected bookings.\n\nAll effects are multiplicative on the count scale. For example, if the coefficient for cleanliness is 0.08, then a one-point increase in cleanliness score is associated with a ( e^{0.08} ) or 8% increase in expected review count.\n\n\n\nSummary\nIn this case study, we analyzed a dataset of 40,000 Airbnb listings in New York City to understand what factors drive the number of reviews a listing receives. After cleaning the data to remove missing values, we used a Poisson regression model to estimate the expected number of reviews as a function of listing attributes such as duration on the platform, price, room type, and review scores.\nThe results suggest that: - Listings that have been active longer, have more bedrooms, and receive higher cleanliness ratings tend to attract more reviews. - Higher prices and shared room types are linked to fewer reviews. - Most effects are statistically significant and directionally plausible.\nOverall, the model highlights actionable levers that hosts might optimize to improve visibility and booking performance on the Airbnb platform. ains variation in review counts with a large sample size (n ≈ 30,000), and all variables except instant_bookable are statistically significant at the 0.001 level."
  }
]